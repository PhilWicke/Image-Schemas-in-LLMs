{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 01\n",
    "## with visual interpretation\n",
    "### Richardson et. al (2002) Experiment 01 with **Vision** Language Models instead of Humans\n",
    "\n",
    "The subjects were presented with a single page,\n",
    "containing a list of the verbs and four pictures, labelled A to\n",
    "D. Each one contained a circle and a square aligned along a\n",
    "vertical or horizontal axis, connected by an arrow pointing\n",
    "up, down, left or right. Since we didn't expect any\n",
    "interesting item variation between left or right placement of\n",
    "the circle or square, the horizontal schemas differed only in\n",
    "the direction of the arrow.\n",
    "For each sentence, subjects were asked to select one of\n",
    "the four sparse images that best depicted the event described\n",
    "by the sentence (Figure 1)\n",
    "The items were randomised in three different orders, and\n",
    "crossed with two different orderings of the images. The six\n",
    "lists were then distributed randomly to subjects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of Experimental Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/pwicke/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image Schemas in LLMs/src/experiment_01/03a_running_exp01.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bepsilon3.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m AutoProcessor, AutoModelForVision2Seq\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bepsilon3.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhuggingface_hub\u001b[39;00m \u001b[39mimport\u001b[39;00m hf_hub_download\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bepsilon3.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mrandom\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bepsilon3.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bepsilon3.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m local_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/mounts/data/corp/huggingface/\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch, random, requests\n",
    "from PIL import Image\n",
    "\n",
    "local_path = \"/mounts/data/corp/huggingface/\"\n",
    "gpu_model_1 = \"cuda:0\"\n",
    "gpu_model_2 = \"cuda:1\"\n",
    "gpu_model_3 = \"cuda:2\"\n",
    "\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading preprocessed data by Richardson\n",
    "\n",
    "Creates three dictionaries:\n",
    " * `richardson_data`\n",
    " \n",
    " All choices as vectors, e.g. `{'fled': [7.2, 4.2, 80.8, 7.8], 'pointed at': [7.2, 3.6, 0.0, 89.2] ...`\n",
    " \n",
    " * `richardson_categorial`\n",
    " \n",
    " Maximum choice as binary choice, e.g. `{'fled': [0, 0, 1, 0], 'pointed at': [0, 0, 0, 1] ...`\n",
    " \n",
    " * `richardson_normed`\n",
    " \n",
    " Maximum choice divided by all choices, disregarding all other choices, e.g.  `{'fled': [0.0, 0.0, 0.808, 0.0], 'pointed at': [0.0, 0.0, 0.0, 0.892] ...`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fled': [0.072, 0.042, 0.808, 0.078], 'pointed at': [0.072, 0.036, 0.0, 0.892], 'pulled': [0.06, 0.054, 0.754, 0.132], 'pushed': [0.072, 0.036, 0.012, 0.88], 'walked': [0.0905, 0.0362, 0.2412, 0.6322], 'hunted': [0.0959, 0.2038, 0.018, 0.6823], 'impacted': [0.072, 0.371, 0.03, 0.527], 'perched': [0.12, 0.76, 0.066, 0.054], 'showed': [0.1499, 0.0899, 0.1019, 0.6583], 'smashed': [0.036, 0.665, 0.012, 0.287], 'bombed': [0.048, 0.868, 0.018, 0.066], 'flew': [0.377, 0.443, 0.15, 0.03], 'floated': [0.329, 0.563, 0.078, 0.03], 'lifted': [0.874, 0.096, 0.024, 0.006], 'sank': [0.2218, 0.7183, 0.042, 0.018], 'argued with': [0.1139, 0.1379, 0.1259, 0.6224], 'gave to': [0.084, 0.096, 0.012, 0.808], 'offended': [0.09, 0.317, 0.246, 0.347], 'rushed': [0.1025, 0.1085, 0.2352, 0.5538], 'warned': [0.1079, 0.2218, 0.0599, 0.6104], 'owned': [0.0539, 0.5564, 0.1858, 0.2038], 'regretted': [0.1978, 0.2398, 0.4126, 0.1499], 'rested': [0.144, 0.365, 0.401, 0.09], 'tempted': [0.168, 0.114, 0.455, 0.263], 'wanted': [0.1558, 0.0779, 0.1558, 0.6104], 'hoped': [0.455, 0.156, 0.072, 0.317], 'increased': [0.7407, 0.0724, 0.0965, 0.0905], 'obeyed': [0.2278, 0.042, 0.6464, 0.0839], 'respected': [0.539, 0.03, 0.144, 0.287], 'succeeded': [0.401, 0.359, 0.108, 0.132]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/richardson_actions.txt\", \"r\") as d_in:\n",
    "    lines = [line.split() for line in d_in.readlines()]\n",
    "\n",
    "output = []\n",
    "for entry in lines:\n",
    "    new_entry = [convert_to_float(item) for item in entry]\n",
    "    \n",
    "    if isinstance(new_entry[1],str):\n",
    "        new_entry[0] = \" \".join(new_entry[:2])\n",
    "        del new_entry[1]\n",
    "    output.append(new_entry)\n",
    "\n",
    "richardson_data = dict()\n",
    "for elem in output:\n",
    "    richardson_data[elem[0]] = [i for i in elem[1:]]\n",
    "\n",
    "# Randomizing Richardson's data\n",
    "action_words = list(richardson_data.keys())\n",
    "random.shuffle(action_words)\n",
    "\n",
    "richardson_categorial = dict()\n",
    "for k, v in richardson_data.items():\n",
    "    if k == 0:\n",
    "        continue\n",
    "    vals = [0,0,0,0]\n",
    "    vals[v.index(max(v))] = 1\n",
    "\n",
    "    richardson_categorial[k] = vals\n",
    "richardson_normed = dict()\n",
    "\n",
    "for action, values in richardson_data.items():\n",
    "    if action == 0:\n",
    "        continue\n",
    "    \n",
    "    richardson_normed[action] = [round(val/sum(values),4) for val in values]\n",
    "\n",
    "print(richardson_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing model\n",
    "\n",
    "### Loading InstructBlip-Vicuna7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:41<00:00, 10.27s/it]\n"
     ]
    }
   ],
   "source": [
    "model_instructblip = InstructBlipForConditionalGeneration.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\")\n",
    "processor_instructblip = InstructBlipProcessor.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\")\n",
    "model_instructblip.to(gpu_model_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# auto : tau 30sec to load \n",
    "model_instructblip = InstructBlipForConditionalGeneration.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\", device_map=\"auto\")\n",
    "processor_instructblip = InstructBlipProcessor.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading OpenFlamingo-9B-vitl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:31<00:00, 10.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1384781840 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Flamingo(\n",
       "  (vision_encoder): VisionTransformer(\n",
       "    (patchnorm_pre_ln): Identity()\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-23): 24 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (perceiver): PerceiverResampler(\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x ModuleList(\n",
       "        (0): PerceiverAttention(\n",
       "          (norm_media): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm_latents): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=1024, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=1024, bias=False)\n",
       "        )\n",
       "        (1): Sequential(\n",
       "          (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lang_encoder): MPTForCausalLM(\n",
       "    (transformer): MPTModel(\n",
       "      (wte): Embedding(50280, 4096)\n",
       "      (emb_drop): Dropout(p=0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-2): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4-6): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8-10): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12-14): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16-18): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20-22): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (24-26): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (27): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (28-30): 3 x FlamingoLayer(\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (31): FlamingoLayer(\n",
       "          (gated_cross_attn_layer): GatedCrossAttentionBlock(\n",
       "            (attn): MaskedCrossAttention(\n",
       "              (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "              (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "            )\n",
       "            (ff): Sequential(\n",
       "              (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "              (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (2): GELU(approximate='none')\n",
       "              (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (decoder_layer): MPTBlock(\n",
       "            (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "              (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (ffn): MPTMLP(\n",
       "              (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "              (act): GELU(approximate='none')\n",
       "              (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "            )\n",
       "            (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "            (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (old_decoder_blocks): ModuleList(\n",
       "      (0-31): 32 x MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (gated_cross_attn_layers): ModuleList(\n",
       "      (0-2): 3 x None\n",
       "      (3): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4-6): 3 x None\n",
       "      (7): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (8-10): 3 x None\n",
       "      (11): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (12-14): 3 x None\n",
       "      (15): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (16-18): 3 x None\n",
       "      (19): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (20-22): 3 x None\n",
       "      (23): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (24-26): 3 x None\n",
       "      (27): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (28-30): 3 x None\n",
       "      (31): GatedCrossAttentionBlock(\n",
       "        (attn): MaskedCrossAttention(\n",
       "          (norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (to_q): Linear(in_features=4096, out_features=512, bias=False)\n",
       "          (to_kv): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (to_out): Linear(in_features=512, out_features=4096, bias=False)\n",
       "        )\n",
       "        (ff): Sequential(\n",
       "          (0): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (2): GELU(approximate='none')\n",
       "          (3): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "model_flamingo, image_processor, tokenizer = create_model_and_transforms(clip_vision_encoder_path=\"ViT-L-14\",clip_vision_encoder_pretrained=\"openai\",lang_encoder_path=\"anas-awadalla/mpt-7b\",tokenizer_path=\"anas-awadalla/mpt-7b\",cross_attn_every_n_layers=4)\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "model_flamingo.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "model_flamingo.to(gpu_model_2) \n",
    "\n",
    "# 7min "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose specifc GPU for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GPU ID you want to use\n",
    "gpu_id = 6\n",
    "\n",
    "# Use the torch.cuda.device() context manager to set the current GPU\n",
    "with torch.cuda.device(gpu_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type+\"/\"+model_name, use_auth_token=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(server_model_path+model_type+\"/\"+model_name, use_auth_token=True).to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ee5e87f83a44378bee09307d166dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpu_id = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(server_model_path+model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision-Language Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Using the exact same image that the participants see (with 4 options A,B,C,D) and the original prompt, the model only ever selects option A.\n",
    "* Shufflinge the images, the model still selects mostly option A. \n",
    "\n",
    "This suggests that the choice is made based upon the name \"A\" and not the image itself. Hence, we encode the images with fake name-codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flew\n",
      "direction02 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle flew square'. The best image is YZPR.<|endofchunk|>\n",
      "\n",
      "\n",
      "hoped\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction02 is YZPR\n",
      "direction03 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle hoped square'. The best image is KYMD.<|endofchunk|>\n",
      "\n",
      "\n",
      "pointed at\n",
      "direction03 is KYMD\n",
      "direction04 is PWTX\n",
      "direction01 is YZPR\n",
      "direction02 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle pointed at square'. The best image is the one which most closely resembles the event 'circle pointed at square'.<|endofchunk|>\n",
      "\n",
      "\n",
      "warned\n",
      "direction01 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction04 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle warned square'. The best image is 'circle warned square'.<|endofchunk|>\n",
      "\n",
      "\n",
      "rested\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction01 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle rested square'. The best image is 'circle rested square'.<|endofchunk|>\n",
      "\n",
      "\n",
      "smashed\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction01 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle smashed square'. The best image is chosen by majority vote.<|endofchunk|>\n",
      "\n",
      "\n",
      "lifted\n",
      "direction02 is KYMD\n",
      "direction04 is PWTX\n",
      "direction01 is YZPR\n",
      "direction03 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle lifted square'. The best image is KYMD.<|endofchunk|>\n",
      "\n",
      "\n",
      "tempted\n",
      "direction03 is KYMD\n",
      "direction04 is PWTX\n",
      "direction02 is YZPR\n",
      "direction01 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle tempted square'. The best image is YZPR.<|endofchunk|>\n",
      "\n",
      "\n",
      "rushed\n",
      "direction01 is KYMD\n",
      "direction02 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle rushed square'. The best image is DHNV.<|endofchunk|>\n",
      "\n",
      "\n",
      "fled\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction01 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle fled square'. The best image is YZPR.<|endofchunk|>\n",
      "\n",
      "\n",
      "increased\n",
      "direction03 is KYMD\n",
      "direction02 is PWTX\n",
      "direction01 is YZPR\n",
      "direction04 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle increased square'. The best image is shown below.<|endofchunk|>\n",
      "\n",
      "\n",
      "offended\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction01 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle offended square'. The best image is the one with the highest number of votes.<|endofchunk|>\n",
      "\n",
      "\n",
      "floated\n",
      "direction02 is KYMD\n",
      "direction03 is PWTX\n",
      "direction04 is YZPR\n",
      "direction01 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle floated square'. The best image is DHNV.<|endofchunk|>\n",
      "\n",
      "\n",
      "sank\n",
      "direction02 is KYMD\n",
      "direction03 is PWTX\n",
      "direction01 is YZPR\n",
      "direction04 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle sank square'. The best image is the one that is most similar to the event. Select the image that best depicts the event 'circle sank square'. The best image is the one that is most similar to the event.<|endofchunk|>\n",
      "\n",
      "\n",
      "gave to\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction02 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle gave to square'. The best image is the one that has the most similarity to the event.<|endofchunk|>\n",
      "\n",
      "\n",
      "owned\n",
      "direction01 is KYMD\n",
      "direction02 is PWTX\n",
      "direction04 is YZPR\n",
      "direction03 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle owned square'. The best image is the one that is closest to the event.<|endofchunk|>\n",
      "\n",
      "\n",
      "showed\n",
      "direction02 is KYMD\n",
      "direction04 is PWTX\n",
      "direction03 is YZPR\n",
      "direction01 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle showed square'. The best image is KYMD.<|endofchunk|>\n",
      "\n",
      "\n",
      "respected\n",
      "direction03 is KYMD\n",
      "direction04 is PWTX\n",
      "direction02 is YZPR\n",
      "direction01 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle respected square'. The best image is 'circle respected square'.<|endofchunk|>\n",
      "\n",
      "\n",
      "bombed\n",
      "direction01 is KYMD\n",
      "direction04 is PWTX\n",
      "direction03 is YZPR\n",
      "direction02 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle bombed square'. The best image is YZPR.<|endofchunk|>\n",
      "\n",
      "\n",
      "succeeded\n",
      "direction01 is KYMD\n",
      "direction02 is PWTX\n",
      "direction04 is YZPR\n",
      "direction03 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle succeeded square'. The best image is 'DHNV'.<|endofchunk|>\n",
      "\n",
      "\n",
      "pulled\n",
      "direction03 is KYMD\n",
      "direction04 is PWTX\n",
      "direction01 is YZPR\n",
      "direction02 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle pulled square'. The best image is KYMD.<|endofchunk|>\n",
      "\n",
      "\n",
      "walked\n",
      "direction01 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction04 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle walked square'. The best image is the one that most closely matches the description.<|endofchunk|>\n",
      "\n",
      "\n",
      "perched\n",
      "direction01 is KYMD\n",
      "direction02 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle perched square'. The best image is KYMD.<|endofchunk|>\n",
      "\n",
      "\n",
      "pushed\n",
      "direction01 is KYMD\n",
      "direction02 is PWTX\n",
      "direction04 is YZPR\n",
      "direction03 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle pushed square'. The best image is the first image.<|endofchunk|>\n",
      "\n",
      "\n",
      "wanted\n",
      "direction02 is KYMD\n",
      "direction01 is PWTX\n",
      "direction04 is YZPR\n",
      "direction03 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle wanted square'. The best image is shown below.<|endofchunk|>\n",
      "\n",
      "\n",
      "regretted\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction02 is YZPR\n",
      "direction03 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle regretted square'. The best image is YZPR.<|endofchunk|>\n",
      "\n",
      "\n",
      "hunted\n",
      "direction02 is KYMD\n",
      "direction04 is PWTX\n",
      "direction01 is YZPR\n",
      "direction03 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle hunted square'. The best image is KYMD.<|endofchunk|>\n",
      "\n",
      "\n",
      "impacted\n",
      "direction03 is KYMD\n",
      "direction01 is PWTX\n",
      "direction02 is YZPR\n",
      "direction04 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle impacted square'. The best image is the first image.<|endofchunk|>\n",
      "\n",
      "\n",
      "obeyed\n",
      "direction01 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction04 is DHNV\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle obeyed square'. The best image is KYMD.<|endofchunk|>\n",
      "\n",
      "\n",
      "argued with\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction02 is DHNV\n",
      "<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle argued with square'. The best image is the first image.<|endofchunk|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_choices = dict()\n",
    "arrows = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "for action_word in action_words:\n",
    "    print(action_word)\n",
    "\n",
    "\n",
    "    # Creating list of images for processing\n",
    "    vision_x = []\n",
    "    letters = [\"KYMD\", \"PWTX\", \"YZPR\", \"DHNV\"]\n",
    "    l = list(range(1,5))\n",
    "    random.shuffle(l)\n",
    "    for idx, selected in enumerate(l):\n",
    "        print(\"direction0\"+str(selected)+\" is \"+letters[idx])\n",
    "        vision_x.append(image_processor(Image.open(\"../../data/direction0\"+str(i)+\".png\").convert(\"RGB\")).unsqueeze(0))\n",
    "\n",
    "    vision_x = torch.cat(vision_x, dim=0)\n",
    "    vision_x = vision_x.unsqueeze(1).unsqueeze(0).to(gpu_model_2)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "\n",
    "    prompt = \"<image>Image KYMD<image>Image PWTX<image>Image YZPR<image>Image DHNV. Select the image that best depicts the event 'circle \"+action_word+\" square'. The best image is\"\n",
    "\n",
    "    lang_x = tokenizer(\n",
    "        [prompt],\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(gpu_model_2)\n",
    "\n",
    "    generated_text = model_flamingo.generate(\n",
    "        vision_x=vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        num_beams=3\n",
    "    )\n",
    "\n",
    "    print(tokenizer.decode(generated_text[0]))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run experiment with InstructBlip\n",
    "We cannot use multiple images with InstructBlip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flew\n",
      "(b) square flew circle\n",
      "\n",
      "hoped\n",
      "(d) square hoped circle\n",
      "\n",
      "pointed at\n",
      "(d) circle pointed at square\n",
      "\n",
      "warned\n",
      "(d) square warned circle\n",
      "\n",
      "rested\n",
      "(d) square rested circle\n",
      "\n",
      "smashed\n",
      "(d) circle smashed square\n",
      "\n",
      "lifted\n",
      "(d) square lifted circle\n",
      "\n",
      "tempted\n",
      "(c) square tempted circle\n",
      "\n",
      "rushed\n",
      "(c) circle rushed square\n",
      "\n",
      "fled\n",
      "(d) square fled circle\n",
      "\n",
      "increased\n",
      "(d) circle increased square\n",
      "\n",
      "offended\n",
      "(d) circle offended square\n",
      "\n",
      "floated\n",
      "(d) square floated circle\n",
      "\n",
      "sank\n",
      "(d) square sank circle\n",
      "\n",
      "gave to\n",
      "(d) circle gave to square\n",
      "\n",
      "owned\n",
      "(c) circle owned square\n",
      "\n",
      "showed\n",
      "d\n",
      "\n",
      "respected\n",
      "(d) square respected circle\n",
      "\n",
      "bombed\n",
      "(d) square bombed circle\n",
      "\n",
      "succeeded\n",
      "(c) square succeeded circle\n",
      "\n",
      "pulled\n",
      "(d) square pulled circle\n",
      "\n",
      "walked\n",
      "(d) square walked circle\n",
      "\n",
      "perched\n",
      "(d) square perched circle\n",
      "\n",
      "pushed\n",
      "(d) square pushed circle\n",
      "\n",
      "wanted\n",
      "(d) square wanted circle\n",
      "\n",
      "regretted\n",
      "(d) square regretted circle\n",
      "\n",
      "hunted\n",
      "d\n",
      "\n",
      "impacted\n",
      "(c) square impacted circle\n",
      "\n",
      "obeyed\n",
      "(d) square obeyed circle\n",
      "\n",
      "argued with\n",
      "(c) square argued with circle\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for action_word in action_words:\n",
    "    print(action_word)\n",
    "    image = Image.open(\"../../data/schema-choice.png\").convert(\"RGB\")\n",
    "    prompt = \"From the image, select the depiction (A, B, C or D) that best describes the event 'circle \"+action_word+\" square'. The best depiction is\"\n",
    "    inputs = processor_instructblip(images=image, text=prompt, return_tensors=\"pt\").to(gpu_model_1)\n",
    "\n",
    "    outputs = model_instructblip.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    num_beams=5,\n",
    "    max_length=256,\n",
    "    min_length=1,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5,\n",
    "    length_penalty=1.0,\n",
    "    temperature=1\n",
    "    )\n",
    "    generated_text = processor_instructblip.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flew\n",
      "bottom-left\n",
      "\n",
      "hoped\n",
      "bottom-left\n",
      "\n",
      "pointed at\n",
      "bottom-left\n",
      "\n",
      "warned\n",
      "bottom-left\n",
      "\n",
      "rested\n",
      "bottom-left\n",
      "\n",
      "smashed\n",
      "bottom-left\n",
      "\n",
      "lifted\n",
      "bottom-left\n",
      "\n",
      "tempted\n",
      "bottom-left\n",
      "\n",
      "rushed\n",
      "bottom-left\n",
      "\n",
      "fled\n",
      "bottom-left\n",
      "\n",
      "increased\n",
      "bottom-left\n",
      "\n",
      "offended\n",
      "bottom-left\n",
      "\n",
      "floated\n",
      "bottom-left\n",
      "\n",
      "sank\n",
      "bottom-left\n",
      "\n",
      "gave to\n",
      "bottom-left\n",
      "\n",
      "owned\n",
      "bottom-left\n",
      "\n",
      "showed\n",
      "bottom-left\n",
      "\n",
      "respected\n",
      "bottom-left\n",
      "\n",
      "bombed\n",
      "bottom-left\n",
      "\n",
      "succeeded\n",
      "bottom-left\n",
      "\n",
      "pulled\n",
      "bottom-left\n",
      "\n",
      "walked\n",
      "bottom-left\n",
      "\n",
      "perched\n",
      "bottom-left\n",
      "\n",
      "pushed\n",
      "bottom-left\n",
      "\n",
      "wanted\n",
      "bottom-left\n",
      "\n",
      "regretted\n",
      "bottom-left\n",
      "\n",
      "hunted\n",
      "bottom-left\n",
      "\n",
      "impacted\n",
      "bottom-left\n",
      "\n",
      "obeyed\n",
      "bottom-left\n",
      "\n",
      "argued with\n",
      "bottom-left\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for action_word in action_words:\n",
    "    print(action_word)\n",
    "    image = Image.open(\"../../data/schema-choice.png\").convert(\"RGB\")\n",
    "    prompt = \"From the image, select the depiction (top-left, top-right, bottom-left or bottom-right) that best describes the event 'circle \"+action_word+\" square'. The best depiction is\"\n",
    "    inputs = processor_instructblip(images=image, text=prompt, return_tensors=\"pt\").to(gpu_model_1)\n",
    "\n",
    "    outputs = model_instructblip.generate(\n",
    "    **inputs,\n",
    "    do_sample=True,\n",
    "    num_beams=5,\n",
    "    max_length=256,\n",
    "    min_length=1,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.5,\n",
    "    length_penalty=1.0,\n",
    "    temperature=1\n",
    "    )\n",
    "    generated_text = processor_instructblip.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n",
    "\n",
    "    print(generated_text)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
