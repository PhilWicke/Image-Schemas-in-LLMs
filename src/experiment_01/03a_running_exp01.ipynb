{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 01\n",
    "## with visual interpretation\n",
    "### Richardson et. al (2002) Experiment 01 with **Vision** Language Models instead of Humans\n",
    "\n",
    "The subjects were presented with a single page,\n",
    "containing a list of the verbs and four pictures, labelled A to\n",
    "D. Each one contained a circle and a square aligned along a\n",
    "vertical or horizontal axis, connected by an arrow pointing\n",
    "up, down, left or right. Since we didn't expect any\n",
    "interesting item variation between left or right placement of\n",
    "the circle or square, the horizontal schemas differed only in\n",
    "the direction of the arrow.\n",
    "For each sentence, subjects were asked to select one of\n",
    "the four sparse images that best depicted the event described\n",
    "by the sentence (Figure 1)\n",
    "The items were randomised in three different orders, and\n",
    "crossed with two different orderings of the images. The six\n",
    "lists were then distributed randomly to subjects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of Experimental Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/pwicke/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch, random\n",
    "from PIL import Image\n",
    "\n",
    "local_path = \"/mounts/data/corp/huggingface/\"\n",
    "gpu_model_1 = \"cuda:6\"\n",
    "gpu_model_2 = \"cuda:7\"\n",
    "\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading preprocessed data by Richardson\n",
    "\n",
    "Creates three dictionaries:\n",
    " * `richardson_data`\n",
    " \n",
    " All choices as vectors, e.g. `{'fled': [7.2, 4.2, 80.8, 7.8], 'pointed at': [7.2, 3.6, 0.0, 89.2] ...`\n",
    " \n",
    " * `richardson_categorial`\n",
    " \n",
    " Maximum choice as binary choice, e.g. `{'fled': [0, 0, 1, 0], 'pointed at': [0, 0, 0, 1] ...`\n",
    " \n",
    " * `richardson_normed`\n",
    " \n",
    " Maximum choice divided by all choices, disregarding all other choices, e.g.  `{'fled': [0.0, 0.0, 0.808, 0.0], 'pointed at': [0.0, 0.0, 0.0, 0.892] ...`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fled': [0.072, 0.042, 0.808, 0.078], 'pointed at': [0.072, 0.036, 0.0, 0.892], 'pulled': [0.06, 0.054, 0.754, 0.132], 'pushed': [0.072, 0.036, 0.012, 0.88], 'walked': [0.0905, 0.0362, 0.2412, 0.6322], 'hunted': [0.0959, 0.2038, 0.018, 0.6823], 'impacted': [0.072, 0.371, 0.03, 0.527], 'perched': [0.12, 0.76, 0.066, 0.054], 'showed': [0.1499, 0.0899, 0.1019, 0.6583], 'smashed': [0.036, 0.665, 0.012, 0.287], 'bombed': [0.048, 0.868, 0.018, 0.066], 'flew': [0.377, 0.443, 0.15, 0.03], 'floated': [0.329, 0.563, 0.078, 0.03], 'lifted': [0.874, 0.096, 0.024, 0.006], 'sank': [0.2218, 0.7183, 0.042, 0.018], 'argued with': [0.1139, 0.1379, 0.1259, 0.6224], 'gave to': [0.084, 0.096, 0.012, 0.808], 'offended': [0.09, 0.317, 0.246, 0.347], 'rushed': [0.1025, 0.1085, 0.2352, 0.5538], 'warned': [0.1079, 0.2218, 0.0599, 0.6104], 'owned': [0.0539, 0.5564, 0.1858, 0.2038], 'regretted': [0.1978, 0.2398, 0.4126, 0.1499], 'rested': [0.144, 0.365, 0.401, 0.09], 'tempted': [0.168, 0.114, 0.455, 0.263], 'wanted': [0.1558, 0.0779, 0.1558, 0.6104], 'hoped': [0.455, 0.156, 0.072, 0.317], 'increased': [0.7407, 0.0724, 0.0965, 0.0905], 'obeyed': [0.2278, 0.042, 0.6464, 0.0839], 'respected': [0.539, 0.03, 0.144, 0.287], 'succeeded': [0.401, 0.359, 0.108, 0.132]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/richardson_actions.txt\", \"r\") as d_in:\n",
    "    lines = [line.split() for line in d_in.readlines()]\n",
    "\n",
    "output = []\n",
    "for entry in lines:\n",
    "    new_entry = [convert_to_float(item) for item in entry]\n",
    "    \n",
    "    if isinstance(new_entry[1],str):\n",
    "        new_entry[0] = \" \".join(new_entry[:2])\n",
    "        del new_entry[1]\n",
    "    output.append(new_entry)\n",
    "\n",
    "richardson_data = dict()\n",
    "for elem in output:\n",
    "    richardson_data[elem[0]] = [i for i in elem[1:]]\n",
    "\n",
    "# Randomizing Richardson's data\n",
    "action_words = list(richardson_data.keys())\n",
    "random.shuffle(action_words)\n",
    "\n",
    "richardson_categorial = dict()\n",
    "for k, v in richardson_data.items():\n",
    "    if k == 0:\n",
    "        continue\n",
    "    vals = [0,0,0,0]\n",
    "    vals[v.index(max(v))] = 1\n",
    "\n",
    "    richardson_categorial[k] = vals\n",
    "richardson_normed = dict()\n",
    "\n",
    "for action, values in richardson_data.items():\n",
    "    if action == 0:\n",
    "        continue\n",
    "    \n",
    "    richardson_normed[action] = [round(val/sum(values),4) for val in values]\n",
    "\n",
    "print(richardson_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing model\n",
    "\n",
    "### Loading InstructBlip-Vicuna7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructblip = InstructBlipForConditionalGeneration.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\")\n",
    "processor_instructblip = InstructBlipProcessor.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\")\n",
    "model_instructblip.to(gpu_model_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards:   0%|          | 0/4 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image Schemas in LLMs/src/experiment_01/03a_running_exp01.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bepsilon7.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# auto\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bepsilon7.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m model_instructblip \u001b[39m=\u001b[39m InstructBlipForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(local_path\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39minstructblip-vicuna/instructblip-vicuna-7b\u001b[39;49m\u001b[39m\"\u001b[39;49m, device_map\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bepsilon7.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m processor_instructblip \u001b[39m=\u001b[39m InstructBlipProcessor\u001b[39m.\u001b[39mfrom_pretrained(local_path\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39minstructblip-vicuna/instructblip-vicuna-7b\u001b[39m\u001b[39m\"\u001b[39m, device_map\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/transformers/modeling_utils.py:2940\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2930\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2931\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   2933\u001b[0m     (\n\u001b[1;32m   2934\u001b[0m         model,\n\u001b[1;32m   2935\u001b[0m         missing_keys,\n\u001b[1;32m   2936\u001b[0m         unexpected_keys,\n\u001b[1;32m   2937\u001b[0m         mismatched_keys,\n\u001b[1;32m   2938\u001b[0m         offload_index,\n\u001b[1;32m   2939\u001b[0m         error_msgs,\n\u001b[0;32m-> 2940\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   2941\u001b[0m         model,\n\u001b[1;32m   2942\u001b[0m         state_dict,\n\u001b[1;32m   2943\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   2944\u001b[0m         resolved_archive_file,\n\u001b[1;32m   2945\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   2946\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   2947\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   2948\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   2949\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   2950\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   2951\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   2952\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   2953\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   2954\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(load_in_8bit \u001b[39mor\u001b[39;49;00m load_in_4bit),\n\u001b[1;32m   2955\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   2956\u001b[0m     )\n\u001b[1;32m   2958\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   2959\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/transformers/modeling_utils.py:3290\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[39mif\u001b[39;00m shard_file \u001b[39min\u001b[39;00m disk_only_shard_files:\n\u001b[1;32m   3289\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m-> 3290\u001b[0m state_dict \u001b[39m=\u001b[39m load_state_dict(shard_file)\n\u001b[1;32m   3292\u001b[0m \u001b[39m# Mistmatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\u001b[39;00m\n\u001b[1;32m   3293\u001b[0m \u001b[39m# matching the weights in the model.\u001b[39;00m\n\u001b[1;32m   3294\u001b[0m mismatched_keys \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _find_mismatched_keys(\n\u001b[1;32m   3295\u001b[0m     state_dict,\n\u001b[1;32m   3296\u001b[0m     model_state_dict,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3300\u001b[0m     ignore_mismatched_sizes,\n\u001b[1;32m   3301\u001b[0m )\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/transformers/modeling_utils.py:464\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m         map_location \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 464\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(checkpoint_file, map_location\u001b[39m=\u001b[39;49mmap_location)\n\u001b[1;32m    465\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    466\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/serialization.py:809\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    808\u001b[0m                 \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    810\u001b[0m \u001b[39mif\u001b[39;00m weights_only:\n\u001b[1;32m    811\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/serialization.py:1172\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1170\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1171\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1172\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1174\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1176\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/serialization.py:1142\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1141\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1142\u001b[0m     typed_storage \u001b[39m=\u001b[39m load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/serialization.py:1112\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_tensor\u001b[39m(dtype, numel, key, location):\n\u001b[1;32m   1110\u001b[0m     name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata/\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 1112\u001b[0m     storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39;49mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39;49mUntypedStorage)\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_untyped_storage\n\u001b[1;32m   1113\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[1;32m   1116\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1117\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m   1118\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# auto\n",
    "model_instructblip = InstructBlipForConditionalGeneration.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\", device_map=\"auto\")\n",
    "processor_instructblip = InstructBlipProcessor.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading OpenFlamingo-9B-vitl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "model_flamingo, image_processor, tokenizer = create_model_and_transforms(clip_vision_encoder_path=\"ViT-L-14\",clip_vision_encoder_pretrained=\"openai\",lang_encoder_path=\"anas-awadalla/mpt-7b\",tokenizer_path=\"anas-awadalla/mpt-7b\",cross_attn_every_n_layers=4)\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "model_flamingo.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "model_flamingo.to(gpu_model_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose specifc GPU for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GPU ID you want to use\n",
    "gpu_id = 6\n",
    "\n",
    "# Use the torch.cuda.device() context manager to set the current GPU\n",
    "with torch.cuda.device(gpu_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type+\"/\"+model_name, use_auth_token=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(server_model_path+model_type+\"/\"+model_name, use_auth_token=True).to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ee5e87f83a44378bee09307d166dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpu_id = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(server_model_path+model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiment (inludes prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:51<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "model_choices = dict()\n",
    "arrows = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "for action_word in tqdm(action_words):\n",
    "\n",
    "    if action_word == 0:\n",
    "        continue\n",
    "\n",
    "    ### PROMPT DEFINED HERE\n",
    "    friendly_prompt = \"Select the CONCEPT that best represents the event described by the sentence: \"+action_word+\". CONCEPTS: UP, DOWN, LEFT, RIGHT.\\nThe best representation is CONCEPT:\"\n",
    "    \n",
    "    if gpu_id:\n",
    "        input_ids = tokenizer.encode(friendly_prompt, return_tensors=\"pt\").to(torch.device(\"cuda\"))\n",
    "        max_length = input_ids.size(1)  + 20\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1).to(torch.device(\"cuda\"))\n",
    "    else:\n",
    "        input_ids = tokenizer.encode(friendly_prompt, return_tensors=\"pt\")\n",
    "        max_length = input_ids.size(1)  + 20\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)  \n",
    "    \n",
    "    model_choices[action_word] = []\n",
    "    for idx, arrow in enumerate(arrows):\n",
    "        if arrow in generated_answer[len(friendly_prompt):]:\n",
    "            model_choices[action_word].append(1)\n",
    "        else:\n",
    "            model_choices[action_word].append(0)\n",
    "\n",
    "# Llama-70b: XXXX\n",
    "# Llama-13b: XXXX\n",
    "# Llama-7b:  50s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing experimental results from free-form generation as `exp01a_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_store = input(\"Should the result for \"+model_name+\" be stored? (y/n):\")\n",
    "\n",
    "if to_store == \"y\":\n",
    "\n",
    "    with open(\"results/exp01b_\"+model_name+\".txt\", \"w\") as f_out:\n",
    "        f_out.write(\"Action\\tUP\\tDOWN\\tLEFT\\tRIGHT\\n\")\n",
    "        for k,v in model_choices.items():\n",
    "            f_out.write(k+\"\\t\"+\"\\t\".join([str(x) for x in v])+\"\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walked': [1, 0, 0, 0],\n",
       " 'perched': [1, 0, 0, 0],\n",
       " 'respected': [1, 0, 0, 0],\n",
       " 'fled': [1, 0, 0, 0],\n",
       " 'pulled': [0, 1, 0, 0],\n",
       " 'wanted': [1, 0, 0, 0],\n",
       " 'pushed': [0, 1, 0, 0],\n",
       " 'argued with': [1, 0, 0, 0],\n",
       " 'obeyed': [1, 0, 0, 0],\n",
       " 'showed': [1, 0, 0, 0],\n",
       " 'sank': [0, 1, 0, 0],\n",
       " 'lifted': [1, 0, 0, 0],\n",
       " 'regretted': [1, 0, 0, 0],\n",
       " 'gave to': [1, 0, 0, 0],\n",
       " 'pointed at': [1, 0, 0, 0],\n",
       " 'succeeded': [1, 0, 0, 0],\n",
       " 'impacted': [1, 0, 0, 0],\n",
       " 'owned': [1, 0, 0, 0],\n",
       " 'smashed': [1, 0, 0, 0],\n",
       " 'increased': [1, 0, 0, 0],\n",
       " 'floated': [1, 0, 0, 0],\n",
       " 'bombed': [0, 1, 0, 0],\n",
       " 'hunted': [1, 0, 0, 0],\n",
       " 'tempted': [1, 0, 0, 0],\n",
       " 'hoped': [1, 0, 0, 0],\n",
       " 'rushed': [1, 0, 0, 0],\n",
       " 'flew': [1, 0, 0, 0],\n",
       " 'rested': [1, 0, 0, 0],\n",
       " 'offended': [1, 0, 0, 0],\n",
       " 'warned': [1, 0, 0, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_choices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
