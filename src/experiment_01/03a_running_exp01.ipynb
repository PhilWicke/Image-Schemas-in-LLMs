{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 01\n",
    "## with visual interpretation\n",
    "### Richardson et. al (2002) Experiment 01 with **Vision** Language Models instead of Humans\n",
    "\n",
    "The subjects were presented with a single page,\n",
    "containing a list of the verbs and four pictures, labelled A to\n",
    "D. Each one contained a circle and a square aligned along a\n",
    "vertical or horizontal axis, connected by an arrow pointing\n",
    "up, down, left or right. Since we didn't expect any\n",
    "interesting item variation between left or right placement of\n",
    "the circle or square, the horizontal schemas differed only in\n",
    "the direction of the arrow.\n",
    "For each sentence, subjects were asked to select one of\n",
    "the four sparse images that best depicted the event described\n",
    "by the sentence (Figure 1)\n",
    "The items were randomised in three different orders, and\n",
    "crossed with two different orderings of the images. The six\n",
    "lists were then distributed randomly to subjects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of Experimental Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/pwicke/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import InstructBlipProcessor, InstructBlipForConditionalGeneration\n",
    "from huggingface_hub import hf_hub_download\n",
    "import torch, random\n",
    "from PIL import Image\n",
    "\n",
    "local_path = \"/mounts/data/corp/huggingface/\"\n",
    "gpu_model_1 = \"cuda:6\"\n",
    "gpu_model_2 = \"cuda:7\"\n",
    "\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading preprocessed data by Richardson\n",
    "\n",
    "Creates three dictionaries:\n",
    " * `richardson_data`\n",
    " \n",
    " All choices as vectors, e.g. `{'fled': [7.2, 4.2, 80.8, 7.8], 'pointed at': [7.2, 3.6, 0.0, 89.2] ...`\n",
    " \n",
    " * `richardson_categorial`\n",
    " \n",
    " Maximum choice as binary choice, e.g. `{'fled': [0, 0, 1, 0], 'pointed at': [0, 0, 0, 1] ...`\n",
    " \n",
    " * `richardson_normed`\n",
    " \n",
    " Maximum choice divided by all choices, disregarding all other choices, e.g.  `{'fled': [0.0, 0.0, 0.808, 0.0], 'pointed at': [0.0, 0.0, 0.0, 0.892] ...`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fled': [0.072, 0.042, 0.808, 0.078], 'pointed at': [0.072, 0.036, 0.0, 0.892], 'pulled': [0.06, 0.054, 0.754, 0.132], 'pushed': [0.072, 0.036, 0.012, 0.88], 'walked': [0.0905, 0.0362, 0.2412, 0.6322], 'hunted': [0.0959, 0.2038, 0.018, 0.6823], 'impacted': [0.072, 0.371, 0.03, 0.527], 'perched': [0.12, 0.76, 0.066, 0.054], 'showed': [0.1499, 0.0899, 0.1019, 0.6583], 'smashed': [0.036, 0.665, 0.012, 0.287], 'bombed': [0.048, 0.868, 0.018, 0.066], 'flew': [0.377, 0.443, 0.15, 0.03], 'floated': [0.329, 0.563, 0.078, 0.03], 'lifted': [0.874, 0.096, 0.024, 0.006], 'sank': [0.2218, 0.7183, 0.042, 0.018], 'argued with': [0.1139, 0.1379, 0.1259, 0.6224], 'gave to': [0.084, 0.096, 0.012, 0.808], 'offended': [0.09, 0.317, 0.246, 0.347], 'rushed': [0.1025, 0.1085, 0.2352, 0.5538], 'warned': [0.1079, 0.2218, 0.0599, 0.6104], 'owned': [0.0539, 0.5564, 0.1858, 0.2038], 'regretted': [0.1978, 0.2398, 0.4126, 0.1499], 'rested': [0.144, 0.365, 0.401, 0.09], 'tempted': [0.168, 0.114, 0.455, 0.263], 'wanted': [0.1558, 0.0779, 0.1558, 0.6104], 'hoped': [0.455, 0.156, 0.072, 0.317], 'increased': [0.7407, 0.0724, 0.0965, 0.0905], 'obeyed': [0.2278, 0.042, 0.6464, 0.0839], 'respected': [0.539, 0.03, 0.144, 0.287], 'succeeded': [0.401, 0.359, 0.108, 0.132]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/richardson_actions.txt\", \"r\") as d_in:\n",
    "    lines = [line.split() for line in d_in.readlines()]\n",
    "\n",
    "output = []\n",
    "for entry in lines:\n",
    "    new_entry = [convert_to_float(item) for item in entry]\n",
    "    \n",
    "    if isinstance(new_entry[1],str):\n",
    "        new_entry[0] = \" \".join(new_entry[:2])\n",
    "        del new_entry[1]\n",
    "    output.append(new_entry)\n",
    "\n",
    "richardson_data = dict()\n",
    "for elem in output:\n",
    "    richardson_data[elem[0]] = [i for i in elem[1:]]\n",
    "\n",
    "# Randomizing Richardson's data\n",
    "action_words = list(richardson_data.keys())\n",
    "random.shuffle(action_words)\n",
    "\n",
    "richardson_categorial = dict()\n",
    "for k, v in richardson_data.items():\n",
    "    if k == 0:\n",
    "        continue\n",
    "    vals = [0,0,0,0]\n",
    "    vals[v.index(max(v))] = 1\n",
    "\n",
    "    richardson_categorial[k] = vals\n",
    "richardson_normed = dict()\n",
    "\n",
    "for action, values in richardson_data.items():\n",
    "    if action == 0:\n",
    "        continue\n",
    "    \n",
    "    richardson_normed[action] = [round(val/sum(values),4) for val in values]\n",
    "\n",
    "print(richardson_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing model\n",
    "\n",
    "### Loading InstructBlip-Vicuna7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instructblip = InstructBlipForConditionalGeneration.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\")\n",
    "processor_instructblip = InstructBlipProcessor.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\")\n",
    "model_instructblip.to(gpu_model_1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:23<00:00,  5.87s/it]\n"
     ]
    }
   ],
   "source": [
    "# auto : tau 30sec to load \n",
    "model_instructblip = InstructBlipForConditionalGeneration.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\", device_map=\"auto\")\n",
    "processor_instructblip = InstructBlipProcessor.from_pretrained(local_path+\"instructblip-vicuna/instructblip-vicuna-7b\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading OpenFlamingo-9B-vitl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [03:56<00:00, 78.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flamingo model initialized with 1384781840 trainable parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['vision_encoder.class_embedding', 'vision_encoder.positional_embedding', 'vision_encoder.proj', 'vision_encoder.conv1.weight', 'vision_encoder.ln_pre.weight', 'vision_encoder.ln_pre.bias', 'vision_encoder.transformer.resblocks.0.ln_1.weight', 'vision_encoder.transformer.resblocks.0.ln_1.bias', 'vision_encoder.transformer.resblocks.0.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.0.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.0.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.0.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.0.ln_2.weight', 'vision_encoder.transformer.resblocks.0.ln_2.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.0.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_1.weight', 'vision_encoder.transformer.resblocks.1.ln_1.bias', 'vision_encoder.transformer.resblocks.1.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.1.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.1.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.1.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.1.ln_2.weight', 'vision_encoder.transformer.resblocks.1.ln_2.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.1.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_1.weight', 'vision_encoder.transformer.resblocks.2.ln_1.bias', 'vision_encoder.transformer.resblocks.2.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.2.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.2.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.2.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.2.ln_2.weight', 'vision_encoder.transformer.resblocks.2.ln_2.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.2.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_1.weight', 'vision_encoder.transformer.resblocks.3.ln_1.bias', 'vision_encoder.transformer.resblocks.3.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.3.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.3.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.3.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.3.ln_2.weight', 'vision_encoder.transformer.resblocks.3.ln_2.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.3.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_1.weight', 'vision_encoder.transformer.resblocks.4.ln_1.bias', 'vision_encoder.transformer.resblocks.4.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.4.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.4.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.4.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.4.ln_2.weight', 'vision_encoder.transformer.resblocks.4.ln_2.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.4.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_1.weight', 'vision_encoder.transformer.resblocks.5.ln_1.bias', 'vision_encoder.transformer.resblocks.5.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.5.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.5.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.5.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.5.ln_2.weight', 'vision_encoder.transformer.resblocks.5.ln_2.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.5.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_1.weight', 'vision_encoder.transformer.resblocks.6.ln_1.bias', 'vision_encoder.transformer.resblocks.6.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.6.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.6.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.6.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.6.ln_2.weight', 'vision_encoder.transformer.resblocks.6.ln_2.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.6.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_1.weight', 'vision_encoder.transformer.resblocks.7.ln_1.bias', 'vision_encoder.transformer.resblocks.7.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.7.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.7.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.7.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.7.ln_2.weight', 'vision_encoder.transformer.resblocks.7.ln_2.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.7.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_1.weight', 'vision_encoder.transformer.resblocks.8.ln_1.bias', 'vision_encoder.transformer.resblocks.8.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.8.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.8.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.8.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.8.ln_2.weight', 'vision_encoder.transformer.resblocks.8.ln_2.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.8.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_1.weight', 'vision_encoder.transformer.resblocks.9.ln_1.bias', 'vision_encoder.transformer.resblocks.9.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.9.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.9.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.9.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.9.ln_2.weight', 'vision_encoder.transformer.resblocks.9.ln_2.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.9.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_1.weight', 'vision_encoder.transformer.resblocks.10.ln_1.bias', 'vision_encoder.transformer.resblocks.10.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.10.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.10.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.10.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.10.ln_2.weight', 'vision_encoder.transformer.resblocks.10.ln_2.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.10.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_1.weight', 'vision_encoder.transformer.resblocks.11.ln_1.bias', 'vision_encoder.transformer.resblocks.11.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.11.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.11.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.11.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.11.ln_2.weight', 'vision_encoder.transformer.resblocks.11.ln_2.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.11.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_1.weight', 'vision_encoder.transformer.resblocks.12.ln_1.bias', 'vision_encoder.transformer.resblocks.12.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.12.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.12.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.12.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.12.ln_2.weight', 'vision_encoder.transformer.resblocks.12.ln_2.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.12.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_1.weight', 'vision_encoder.transformer.resblocks.13.ln_1.bias', 'vision_encoder.transformer.resblocks.13.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.13.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.13.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.13.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.13.ln_2.weight', 'vision_encoder.transformer.resblocks.13.ln_2.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.13.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_1.weight', 'vision_encoder.transformer.resblocks.14.ln_1.bias', 'vision_encoder.transformer.resblocks.14.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.14.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.14.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.14.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.14.ln_2.weight', 'vision_encoder.transformer.resblocks.14.ln_2.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.14.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_1.weight', 'vision_encoder.transformer.resblocks.15.ln_1.bias', 'vision_encoder.transformer.resblocks.15.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.15.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.15.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.15.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.15.ln_2.weight', 'vision_encoder.transformer.resblocks.15.ln_2.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.15.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_1.weight', 'vision_encoder.transformer.resblocks.16.ln_1.bias', 'vision_encoder.transformer.resblocks.16.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.16.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.16.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.16.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.16.ln_2.weight', 'vision_encoder.transformer.resblocks.16.ln_2.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.16.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_1.weight', 'vision_encoder.transformer.resblocks.17.ln_1.bias', 'vision_encoder.transformer.resblocks.17.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.17.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.17.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.17.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.17.ln_2.weight', 'vision_encoder.transformer.resblocks.17.ln_2.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.17.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_1.weight', 'vision_encoder.transformer.resblocks.18.ln_1.bias', 'vision_encoder.transformer.resblocks.18.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.18.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.18.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.18.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.18.ln_2.weight', 'vision_encoder.transformer.resblocks.18.ln_2.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.18.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_1.weight', 'vision_encoder.transformer.resblocks.19.ln_1.bias', 'vision_encoder.transformer.resblocks.19.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.19.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.19.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.19.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.19.ln_2.weight', 'vision_encoder.transformer.resblocks.19.ln_2.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.19.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_1.weight', 'vision_encoder.transformer.resblocks.20.ln_1.bias', 'vision_encoder.transformer.resblocks.20.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.20.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.20.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.20.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.20.ln_2.weight', 'vision_encoder.transformer.resblocks.20.ln_2.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.20.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_1.weight', 'vision_encoder.transformer.resblocks.21.ln_1.bias', 'vision_encoder.transformer.resblocks.21.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.21.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.21.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.21.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.21.ln_2.weight', 'vision_encoder.transformer.resblocks.21.ln_2.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.21.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_1.weight', 'vision_encoder.transformer.resblocks.22.ln_1.bias', 'vision_encoder.transformer.resblocks.22.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.22.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.22.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.22.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.22.ln_2.weight', 'vision_encoder.transformer.resblocks.22.ln_2.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.22.mlp.c_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_1.weight', 'vision_encoder.transformer.resblocks.23.ln_1.bias', 'vision_encoder.transformer.resblocks.23.attn.in_proj_weight', 'vision_encoder.transformer.resblocks.23.attn.in_proj_bias', 'vision_encoder.transformer.resblocks.23.attn.out_proj.weight', 'vision_encoder.transformer.resblocks.23.attn.out_proj.bias', 'vision_encoder.transformer.resblocks.23.ln_2.weight', 'vision_encoder.transformer.resblocks.23.ln_2.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_fc.bias', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.weight', 'vision_encoder.transformer.resblocks.23.mlp.c_proj.bias', 'vision_encoder.ln_post.weight', 'vision_encoder.ln_post.bias', 'lang_encoder.transformer.blocks.0.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.0.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.1.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.2.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.3.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.4.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.5.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.6.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.7.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.8.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.9.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.10.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.11.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.12.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.13.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.14.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.15.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.16.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.17.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.18.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.19.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.20.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.21.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.22.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.23.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.24.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.25.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.26.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.27.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.28.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.29.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.30.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.norm_1.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.attn.Wqkv.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.attn.out_proj.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.norm_2.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.ffn.up_proj.weight', 'lang_encoder.transformer.blocks.31.decoder_layer.ffn.down_proj.weight', 'lang_encoder.transformer.norm_f.weight', 'lang_encoder.old_decoder_blocks.0.norm_1.weight', 'lang_encoder.old_decoder_blocks.0.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.0.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.0.norm_2.weight', 'lang_encoder.old_decoder_blocks.0.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.0.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.1.norm_1.weight', 'lang_encoder.old_decoder_blocks.1.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.1.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.1.norm_2.weight', 'lang_encoder.old_decoder_blocks.1.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.1.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.2.norm_1.weight', 'lang_encoder.old_decoder_blocks.2.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.2.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.2.norm_2.weight', 'lang_encoder.old_decoder_blocks.2.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.2.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.3.norm_1.weight', 'lang_encoder.old_decoder_blocks.3.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.3.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.3.norm_2.weight', 'lang_encoder.old_decoder_blocks.3.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.3.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.4.norm_1.weight', 'lang_encoder.old_decoder_blocks.4.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.4.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.4.norm_2.weight', 'lang_encoder.old_decoder_blocks.4.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.4.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.5.norm_1.weight', 'lang_encoder.old_decoder_blocks.5.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.5.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.5.norm_2.weight', 'lang_encoder.old_decoder_blocks.5.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.5.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.6.norm_1.weight', 'lang_encoder.old_decoder_blocks.6.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.6.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.6.norm_2.weight', 'lang_encoder.old_decoder_blocks.6.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.6.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.7.norm_1.weight', 'lang_encoder.old_decoder_blocks.7.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.7.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.7.norm_2.weight', 'lang_encoder.old_decoder_blocks.7.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.7.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.8.norm_1.weight', 'lang_encoder.old_decoder_blocks.8.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.8.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.8.norm_2.weight', 'lang_encoder.old_decoder_blocks.8.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.8.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.9.norm_1.weight', 'lang_encoder.old_decoder_blocks.9.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.9.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.9.norm_2.weight', 'lang_encoder.old_decoder_blocks.9.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.9.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.10.norm_1.weight', 'lang_encoder.old_decoder_blocks.10.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.10.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.10.norm_2.weight', 'lang_encoder.old_decoder_blocks.10.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.10.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.11.norm_1.weight', 'lang_encoder.old_decoder_blocks.11.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.11.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.11.norm_2.weight', 'lang_encoder.old_decoder_blocks.11.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.11.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.12.norm_1.weight', 'lang_encoder.old_decoder_blocks.12.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.12.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.12.norm_2.weight', 'lang_encoder.old_decoder_blocks.12.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.12.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.13.norm_1.weight', 'lang_encoder.old_decoder_blocks.13.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.13.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.13.norm_2.weight', 'lang_encoder.old_decoder_blocks.13.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.13.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.14.norm_1.weight', 'lang_encoder.old_decoder_blocks.14.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.14.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.14.norm_2.weight', 'lang_encoder.old_decoder_blocks.14.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.14.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.15.norm_1.weight', 'lang_encoder.old_decoder_blocks.15.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.15.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.15.norm_2.weight', 'lang_encoder.old_decoder_blocks.15.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.15.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.16.norm_1.weight', 'lang_encoder.old_decoder_blocks.16.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.16.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.16.norm_2.weight', 'lang_encoder.old_decoder_blocks.16.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.16.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.17.norm_1.weight', 'lang_encoder.old_decoder_blocks.17.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.17.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.17.norm_2.weight', 'lang_encoder.old_decoder_blocks.17.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.17.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.18.norm_1.weight', 'lang_encoder.old_decoder_blocks.18.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.18.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.18.norm_2.weight', 'lang_encoder.old_decoder_blocks.18.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.18.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.19.norm_1.weight', 'lang_encoder.old_decoder_blocks.19.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.19.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.19.norm_2.weight', 'lang_encoder.old_decoder_blocks.19.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.19.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.20.norm_1.weight', 'lang_encoder.old_decoder_blocks.20.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.20.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.20.norm_2.weight', 'lang_encoder.old_decoder_blocks.20.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.20.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.21.norm_1.weight', 'lang_encoder.old_decoder_blocks.21.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.21.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.21.norm_2.weight', 'lang_encoder.old_decoder_blocks.21.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.21.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.22.norm_1.weight', 'lang_encoder.old_decoder_blocks.22.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.22.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.22.norm_2.weight', 'lang_encoder.old_decoder_blocks.22.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.22.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.23.norm_1.weight', 'lang_encoder.old_decoder_blocks.23.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.23.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.23.norm_2.weight', 'lang_encoder.old_decoder_blocks.23.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.23.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.24.norm_1.weight', 'lang_encoder.old_decoder_blocks.24.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.24.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.24.norm_2.weight', 'lang_encoder.old_decoder_blocks.24.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.24.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.25.norm_1.weight', 'lang_encoder.old_decoder_blocks.25.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.25.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.25.norm_2.weight', 'lang_encoder.old_decoder_blocks.25.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.25.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.26.norm_1.weight', 'lang_encoder.old_decoder_blocks.26.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.26.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.26.norm_2.weight', 'lang_encoder.old_decoder_blocks.26.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.26.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.27.norm_1.weight', 'lang_encoder.old_decoder_blocks.27.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.27.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.27.norm_2.weight', 'lang_encoder.old_decoder_blocks.27.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.27.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.28.norm_1.weight', 'lang_encoder.old_decoder_blocks.28.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.28.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.28.norm_2.weight', 'lang_encoder.old_decoder_blocks.28.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.28.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.29.norm_1.weight', 'lang_encoder.old_decoder_blocks.29.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.29.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.29.norm_2.weight', 'lang_encoder.old_decoder_blocks.29.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.29.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.30.norm_1.weight', 'lang_encoder.old_decoder_blocks.30.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.30.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.30.norm_2.weight', 'lang_encoder.old_decoder_blocks.30.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.30.ffn.down_proj.weight', 'lang_encoder.old_decoder_blocks.31.norm_1.weight', 'lang_encoder.old_decoder_blocks.31.attn.Wqkv.weight', 'lang_encoder.old_decoder_blocks.31.attn.out_proj.weight', 'lang_encoder.old_decoder_blocks.31.norm_2.weight', 'lang_encoder.old_decoder_blocks.31.ffn.up_proj.weight', 'lang_encoder.old_decoder_blocks.31.ffn.down_proj.weight', 'lang_encoder.gated_cross_attn_layers.3.attn_gate', 'lang_encoder.gated_cross_attn_layers.3.ff_gate', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.3.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.3.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.3.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.3.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.7.attn_gate', 'lang_encoder.gated_cross_attn_layers.7.ff_gate', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.7.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.7.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.7.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.7.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.11.attn_gate', 'lang_encoder.gated_cross_attn_layers.11.ff_gate', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.11.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.11.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.11.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.11.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.15.attn_gate', 'lang_encoder.gated_cross_attn_layers.15.ff_gate', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.15.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.15.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.15.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.15.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.19.attn_gate', 'lang_encoder.gated_cross_attn_layers.19.ff_gate', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.19.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.19.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.19.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.19.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.23.attn_gate', 'lang_encoder.gated_cross_attn_layers.23.ff_gate', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.23.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.23.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.23.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.23.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.27.attn_gate', 'lang_encoder.gated_cross_attn_layers.27.ff_gate', 'lang_encoder.gated_cross_attn_layers.27.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.27.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.27.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.27.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.27.ff.3.weight', 'lang_encoder.gated_cross_attn_layers.31.attn_gate', 'lang_encoder.gated_cross_attn_layers.31.ff_gate', 'lang_encoder.gated_cross_attn_layers.31.attn.norm.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.norm.bias', 'lang_encoder.gated_cross_attn_layers.31.attn.to_q.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.to_kv.weight', 'lang_encoder.gated_cross_attn_layers.31.attn.to_out.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.0.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.0.bias', 'lang_encoder.gated_cross_attn_layers.31.ff.1.weight', 'lang_encoder.gated_cross_attn_layers.31.ff.3.weight'], unexpected_keys=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from open_flamingo import create_model_and_transforms\n",
    "model_flamingo, image_processor, tokenizer = create_model_and_transforms(clip_vision_encoder_path=\"ViT-L-14\",clip_vision_encoder_pretrained=\"openai\",lang_encoder_path=\"anas-awadalla/mpt-7b\",tokenizer_path=\"anas-awadalla/mpt-7b\",cross_attn_every_n_layers=4)\n",
    "checkpoint_path = hf_hub_download(\"openflamingo/OpenFlamingo-9B-vitl-mpt7b\", \"checkpoint.pt\")\n",
    "model_flamingo.load_state_dict(torch.load(checkpoint_path), strict=False)\n",
    "#model_flamingo.to(gpu_model_2) \n",
    "\n",
    "# 7min "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose specifc GPU for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GPU ID you want to use\n",
    "gpu_id = 6\n",
    "\n",
    "# Use the torch.cuda.device() context manager to set the current GPU\n",
    "with torch.cuda.device(gpu_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type+\"/\"+model_name, use_auth_token=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(server_model_path+model_type+\"/\"+model_name, use_auth_token=True).to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ee5e87f83a44378bee09307d166dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpu_id = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(server_model_path+model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision-Lanuage Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walked\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118,  7428,  6278,     8, 50277]])\n",
      "owned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118,  9633,  6278,     8, 50277]])\n",
      "wanted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118,  3078,  6278,     8, 50277]])\n",
      "pointed at\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118,  8042,   387,  6278,     8, 50277]])\n",
      "argued with\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118,  9125,   342,  6278,     8,   285,  3662,   253,\n",
      "          1953,    15, 50277]])\n",
      "flew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118, 18811,  6278,     8, 50277]])\n",
      "rested\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118, 27001,  6278,     8, 50277]])\n",
      "pushed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118, 10184,  6278,     8, 50277]])\n",
      "hunted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118, 47214,  6278,     8,   275,   253, 10659,    15,\n",
      "         50277]])\n",
      "pulled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118,  7320,  6278,     8,   347,   352,  5369,  3502,\n",
      "           275,   673,    15, 50277]])\n",
      "showed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118,  2692,  6278,     8,   275,   253,  1563,  3425,\n",
      "            15, 50277]])\n",
      "obeyed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118, 20090,   264,  6278,     8, 50277]])\n",
      "hoped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118, 13937,  6278,     8, 50277]])\n",
      "increased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118,  2559,  6278,     8, 50277]])\n",
      "rushed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118, 20906,  6278,     8,   432,   253,  4677,  1840,\n",
      "            15, 50277]])\n",
      "lifted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118, 14287,  6278,     8,   275,   253, 10659,    15,\n",
      "         50277]])\n",
      "impacted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[50278, 10004,   581,   273,   253,  1740,  3888,   313,    34,    13,\n",
      "           378,    13,   330,    13,   399,    10,   326,  1682, 31444,   253,\n",
      "          2362,   686, 23118, 27857,  6278,     8, 50277]])\n",
      "fled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50277 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image Schemas in LLMs/src/experiment_01/03a_running_exp01.ipynb Cell 16\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSelect one of the four images (A, B, C, D) that best depicts the event \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcircle \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39maction_word\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m square\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m lang_x \u001b[39m=\u001b[39m tokenizer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39m<image>\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mprompt],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m )\u001b[39m#.to(gpu_model_2)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m generated_text \u001b[39m=\u001b[39m model_flamingo\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     vision_x\u001b[39m=\u001b[39;49mvision_x,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m     lang_x\u001b[39m=\u001b[39;49mlang_x[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mlang_x[\u001b[39m\"\u001b[39;49m\u001b[39mattention_mask\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     num_beams\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Btau.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/03a_running_exp01.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(generated_text)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/open_flamingo/src/flamingo.py:174\u001b[0m, in \u001b[0;36mFlamingo.generate\u001b[0;34m(self, vision_x, lang_x, attention_mask, num_beams, min_new_tokens, max_new_tokens, temperature, top_k, top_p, no_repeat_ngram_size, repetition_penalty, prefix_allowed_tokens_fn, length_penalty, num_return_sequences, do_sample, early_stopping)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang_encoder\u001b[39m.\u001b[39m_use_cached_vision_x \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_vision_x(vision_x\u001b[39m=\u001b[39mvision_x)\n\u001b[0;32m--> 174\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlang_encoder\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m    175\u001b[0m     input_ids\u001b[39m=\u001b[39;49mlang_x,\n\u001b[1;32m    176\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    177\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meoc_token_id,\n\u001b[1;32m    178\u001b[0m     num_beams\u001b[39m=\u001b[39;49mnum_beams,\n\u001b[1;32m    179\u001b[0m     min_new_tokens\u001b[39m=\u001b[39;49mmin_new_tokens,\n\u001b[1;32m    180\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m    181\u001b[0m     temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m    182\u001b[0m     top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m    183\u001b[0m     top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m    184\u001b[0m     prefix_allowed_tokens_fn\u001b[39m=\u001b[39;49mprefix_allowed_tokens_fn,\n\u001b[1;32m    185\u001b[0m     no_repeat_ngram_size\u001b[39m=\u001b[39;49mno_repeat_ngram_size,\n\u001b[1;32m    186\u001b[0m     repetition_penalty\u001b[39m=\u001b[39;49mrepetition_penalty,\n\u001b[1;32m    187\u001b[0m     length_penalty\u001b[39m=\u001b[39;49mlength_penalty,\n\u001b[1;32m    188\u001b[0m     num_return_sequences\u001b[39m=\u001b[39;49mnum_return_sequences,\n\u001b[1;32m    189\u001b[0m     do_sample\u001b[39m=\u001b[39;49mdo_sample,\n\u001b[1;32m    190\u001b[0m     early_stopping\u001b[39m=\u001b[39;49mearly_stopping,\n\u001b[1;32m    191\u001b[0m )\n\u001b[1;32m    193\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang_encoder\u001b[39m.\u001b[39mclear_conditioned_layers()\n\u001b[1;32m    194\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlang_encoder\u001b[39m.\u001b[39m_use_cached_vision_x \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/transformers/generation/utils.py:1684\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1676\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1677\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1678\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams \u001b[39m*\u001b[39m generation_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1679\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1680\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1681\u001b[0m     )\n\u001b[1;32m   1683\u001b[0m     \u001b[39m# 14. run beam sample\u001b[39;00m\n\u001b[0;32m-> 1684\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeam_sample(\n\u001b[1;32m   1685\u001b[0m         input_ids,\n\u001b[1;32m   1686\u001b[0m         beam_scorer,\n\u001b[1;32m   1687\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1688\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1689\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1690\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1691\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1692\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1693\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1694\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1695\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1696\u001b[0m     )\n\u001b[1;32m   1698\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGROUP_BEAM_SEARCH:\n\u001b[1;32m   1699\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1701\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1702\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1708\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1709\u001b[0m     )\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/transformers/generation/utils.py:3310\u001b[0m, in \u001b[0;36mGenerationMixin.beam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3306\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m   3308\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[0;32m-> 3310\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   3311\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   3312\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   3313\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   3314\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   3315\u001b[0m )\n\u001b[1;32m   3317\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   3318\u001b[0m     cur_len \u001b[39m=\u001b[39m cur_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/open_flamingo/src/flamingo_lm.py:157\u001b[0m, in \u001b[0;36mFlamingoLMMixin.forward\u001b[0;34m(self, input_ids, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m input_ids\n\u001b[1;32m    156\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m attention_mask\n\u001b[0;32m--> 157\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mounts/data/corp/huggingface/modules/transformers_modules/anas-awadalla/mpt-7b/b772e556c8e8a17d087db6935e7cd019e5eefb0f/modeling_mpt.py:254\u001b[0m, in \u001b[0;36mMPTForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, prefix_mask, sequence_id, labels, return_dict, output_attentions, output_hidden_states, use_cache)\u001b[0m\n\u001b[1;32m    252\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mreturn_dict\n\u001b[1;32m    253\u001b[0m use_cache \u001b[39m=\u001b[39m use_cache \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache\n\u001b[0;32m--> 254\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(input_ids\u001b[39m=\u001b[39;49minput_ids, past_key_values\u001b[39m=\u001b[39;49mpast_key_values, attention_mask\u001b[39m=\u001b[39;49mattention_mask, prefix_mask\u001b[39m=\u001b[39;49mprefix_mask, sequence_id\u001b[39m=\u001b[39;49msequence_id, return_dict\u001b[39m=\u001b[39;49mreturn_dict, output_attentions\u001b[39m=\u001b[39;49moutput_attentions, output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states, use_cache\u001b[39m=\u001b[39;49muse_cache)\n\u001b[1;32m    255\u001b[0m logits \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mlinear(outputs\u001b[39m.\u001b[39mlast_hidden_state\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mwte\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mwte\u001b[39m.\u001b[39mweight)\n\u001b[1;32m    256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogit_scale \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mounts/data/corp/huggingface/modules/transformers_modules/anas-awadalla/mpt-7b/b772e556c8e8a17d087db6935e7cd019e5eefb0f/modeling_mpt.py:194\u001b[0m, in \u001b[0;36mMPTModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, prefix_mask, sequence_id, return_dict, output_attentions, output_hidden_states, use_cache)\u001b[0m\n\u001b[1;32m    192\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (x,)\n\u001b[1;32m    193\u001b[0m past_key_value \u001b[39m=\u001b[39m past_key_values[b_idx] \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m (x, attn_weights, past_key_value) \u001b[39m=\u001b[39m block(x, past_key_value\u001b[39m=\u001b[39;49mpast_key_value, attn_bias\u001b[39m=\u001b[39;49mattn_bias, attention_mask\u001b[39m=\u001b[39;49mattention_mask, is_causal\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_causal)\n\u001b[1;32m    195\u001b[0m \u001b[39mif\u001b[39;00m past_key_values \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m     past_key_values[b_idx] \u001b[39m=\u001b[39m past_key_value\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/open_flamingo/src/flamingo_lm.py:63\u001b[0m, in \u001b[0;36mFlamingoLayer.forward\u001b[0;34m(self, lang_x, attention_mask, **decoder_layer_kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m     lang_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgated_cross_attn_layer(\n\u001b[1;32m     56\u001b[0m         lang_x,\n\u001b[1;32m     57\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvis_x,\n\u001b[1;32m     58\u001b[0m         media_locations\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmedia_locations,\n\u001b[1;32m     59\u001b[0m         use_cached_media\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_cached_media,\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39m# Normal decoder layer\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m lang_x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder_layer(\n\u001b[1;32m     64\u001b[0m     lang_x, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdecoder_layer_kwargs\n\u001b[1;32m     65\u001b[0m )\n\u001b[1;32m     66\u001b[0m \u001b[39mreturn\u001b[39;00m lang_x\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mounts/data/corp/huggingface/modules/transformers_modules/anas-awadalla/mpt-7b/b772e556c8e8a17d087db6935e7cd019e5eefb0f/blocks.py:39\u001b[0m, in \u001b[0;36mMPTBlock.forward\u001b[0;34m(self, x, past_key_value, attn_bias, attention_mask, is_causal)\u001b[0m\n\u001b[1;32m     37\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_attn_dropout(b)\n\u001b[1;32m     38\u001b[0m m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm_2(x)\n\u001b[0;32m---> 39\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mffn(m)\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_ffn_dropout(n)\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m (x, attn_weights, past_key_value)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mounts/data/corp/huggingface/modules/transformers_modules/anas-awadalla/mpt-7b/b772e556c8e8a17d087db6935e7cd019e5eefb0f/blocks.py:18\u001b[0m, in \u001b[0;36mMPTMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 18\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdown_proj(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mup_proj(x)))\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/instructblip_test/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_choices = dict()\n",
    "arrows = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "img_path = \"../../data/schema-choice.png\"\n",
    "img_01 = Image.open(img_path).convert(\"RGB\")\n",
    "vision_x = [image_processor(img_01).unsqueeze(0)]\n",
    "vision_x = torch.cat(vision_x, dim=0)\n",
    "vision_x = vision_x.unsqueeze(1).unsqueeze(0)#.to(gpu_model_2)\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "for action_word in action_words:\n",
    "    print(action_word)\n",
    "\n",
    "    prompt = \"Select one of the four images (A, B, C, D) that best depicts the event 'circle \"+action_word+\" square'\"\n",
    "\n",
    "    lang_x = tokenizer(\n",
    "        [\"<image>\"+prompt],\n",
    "        return_tensors=\"pt\",\n",
    "    )#.to(gpu_model_2)\n",
    "\n",
    "    generated_text = model_flamingo.generate(\n",
    "        vision_x=vision_x,\n",
    "        lang_x=lang_x[\"input_ids\"],\n",
    "        attention_mask=lang_x[\"attention_mask\"],\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        num_beams=3\n",
    "    )\n",
    "\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiment (inludes prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:51<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "model_choices = dict()\n",
    "arrows = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "for action_word in tqdm(action_words):\n",
    "\n",
    "    if action_word == 0:\n",
    "        continue\n",
    "\n",
    "    ### PROMPT DEFINED HERE\n",
    "    friendly_prompt = \"Select the CONCEPT that best represents the event described by the sentence: \"+action_word+\". CONCEPTS: UP, DOWN, LEFT, RIGHT.\\nThe best representation is CONCEPT:\"\n",
    "    \n",
    "    if gpu_id:\n",
    "        input_ids = tokenizer.encode(friendly_prompt, return_tensors=\"pt\").to(torch.device(\"cuda\"))\n",
    "        max_length = input_ids.size(1)  + 20\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1).to(torch.device(\"cuda\"))\n",
    "    else:\n",
    "        input_ids = tokenizer.encode(friendly_prompt, return_tensors=\"pt\")\n",
    "        max_length = input_ids.size(1)  + 20\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)  \n",
    "    \n",
    "    model_choices[action_word] = []\n",
    "    for idx, arrow in enumerate(arrows):\n",
    "        if arrow in generated_answer[len(friendly_prompt):]:\n",
    "            model_choices[action_word].append(1)\n",
    "        else:\n",
    "            model_choices[action_word].append(0)\n",
    "\n",
    "# Llama-70b: XXXX\n",
    "# Llama-13b: XXXX\n",
    "# Llama-7b:  50s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing experimental results from free-form generation as `exp01a_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_store = input(\"Should the result for \"+model_name+\" be stored? (y/n):\")\n",
    "\n",
    "if to_store == \"y\":\n",
    "\n",
    "    with open(\"results/exp01b_\"+model_name+\".txt\", \"w\") as f_out:\n",
    "        f_out.write(\"Action\\tUP\\tDOWN\\tLEFT\\tRIGHT\\n\")\n",
    "        for k,v in model_choices.items():\n",
    "            f_out.write(k+\"\\t\"+\"\\t\".join([str(x) for x in v])+\"\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walked': [1, 0, 0, 0],\n",
       " 'perched': [1, 0, 0, 0],\n",
       " 'respected': [1, 0, 0, 0],\n",
       " 'fled': [1, 0, 0, 0],\n",
       " 'pulled': [0, 1, 0, 0],\n",
       " 'wanted': [1, 0, 0, 0],\n",
       " 'pushed': [0, 1, 0, 0],\n",
       " 'argued with': [1, 0, 0, 0],\n",
       " 'obeyed': [1, 0, 0, 0],\n",
       " 'showed': [1, 0, 0, 0],\n",
       " 'sank': [0, 1, 0, 0],\n",
       " 'lifted': [1, 0, 0, 0],\n",
       " 'regretted': [1, 0, 0, 0],\n",
       " 'gave to': [1, 0, 0, 0],\n",
       " 'pointed at': [1, 0, 0, 0],\n",
       " 'succeeded': [1, 0, 0, 0],\n",
       " 'impacted': [1, 0, 0, 0],\n",
       " 'owned': [1, 0, 0, 0],\n",
       " 'smashed': [1, 0, 0, 0],\n",
       " 'increased': [1, 0, 0, 0],\n",
       " 'floated': [1, 0, 0, 0],\n",
       " 'bombed': [0, 1, 0, 0],\n",
       " 'hunted': [1, 0, 0, 0],\n",
       " 'tempted': [1, 0, 0, 0],\n",
       " 'hoped': [1, 0, 0, 0],\n",
       " 'rushed': [1, 0, 0, 0],\n",
       " 'flew': [1, 0, 0, 0],\n",
       " 'rested': [1, 0, 0, 0],\n",
       " 'offended': [1, 0, 0, 0],\n",
       " 'warned': [1, 0, 0, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_choices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
