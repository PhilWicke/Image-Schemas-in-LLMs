{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method by Aher et. al (2023) to pick best prompt ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import itertools\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from sidemethods import logprobs_from_prompt, proc, proc_lower, prob_of_ending, load_richardson_data\n",
    "server_model_path = \"/mounts/data/corp/huggingface/\"\n",
    "\n",
    "\n",
    "# loading the original human data as vectors for each action word\n",
    "_, richardson_data, richardson_normed = load_richardson_data()\n",
    "action_words = richardson_normed.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model, functions and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /mounts/data/corp/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "with open(\"../../hf.key\", \"r\") as f_in:\n",
    "    hf_key = f_in.readline().strip()\n",
    "login(token = hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "#model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "#model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "#model_name = \"meta-llama/Llama-2-70b-hf\"\n",
    "#model_name = \"meta-llama/Llama-2-70b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f91836f3034f1e8e1e60d8d911024a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the GPU ID you want to use\n",
    "gpu_id = 1\n",
    "\n",
    "# Use the torch.cuda.device() context manager to set the current GPU\n",
    "with torch.cuda.device(gpu_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) # use_auth_token=True\n",
    "    model = AutoModelForCausalLM.from_pretrained(server_model_path+model_name).to(torch.device(\"cuda\")) # use_auth_token=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_sum(input_samples, model, tokenizer, prompt_ending, hf):\n",
    "    '''\n",
    "    compute the sum of log probabilities attributed to the 7 allowed scores over x=len(input_samples) samples\n",
    "\n",
    "    input_samples: list of prompts\n",
    "    model: the model to use, either a string of a gpt model or a loaded huggingface model object\n",
    "    tokenizer: huggingface tokenizer object\n",
    "    prompt_ending: the string that is appended to the prompt\n",
    "    hf: whether to use huggingface or gpt\n",
    "    '''\n",
    "    proba_sum = 0\n",
    "    avg_output_list = []    #this contains probability-averaged likert scores per prompt\n",
    "    for prompt in tqdm(input_samples):\n",
    "        if hf:\n",
    "            proba_list, avg_likert, _ = hf_complete_proba(prompt=prompt, model=model, tokenizer=tokenizer, prompt_ending=prompt_ending, verbose=False)\n",
    "        else:\n",
    "            proba_list, avg_likert, debug = gpt_complete_proba(prompt, model, prompt_ending, verbose=True)\n",
    "        proba_sum += sum(proba_list)\n",
    "        avg_output_list.append(avg_likert)\n",
    "    return proba_sum, avg_output_list       # return sum and the list of averaged likert scores per prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the prompt to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Given the event 'fled', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'pointed at', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'pulled', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'pushed', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'walked', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'hunted', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'impacted', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'perched', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'showed', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'smashed', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'bombed', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'flew', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'floated', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'lifted', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'sank', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'argued with', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'gave to', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'offended', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'rushed', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'warned', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'owned', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'regretted', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'rested', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'tempted', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'wanted', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'hoped', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'increased', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'obeyed', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'respected', which of the following arrows best represents this event: ↑, ↓, ←, →.\", \"Given the event 'succeeded', which of the following arrows best represents this event: ↑, ↓, ←, →.\"]\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL VARIABLES (ugly)\n",
    "test_inputs = []\n",
    "arrows = ['↑', '↓', '←', '→']\n",
    "answers = {0:'↑', 1:'↓', 2:'←', 3:'→'}\n",
    "\n",
    "for word in list(action_words):\n",
    "    # test_inputs.append(\"Given the event '\"+word+\"', which of the following arrows best represents this concept: \"+\", \".join(arrow_list)+\".\")\n",
    "    # test_inputs.append(\"Given the event '\"+word+\"', which of the following arrows best represents this concept: '↑', '↓', '←', '→'.\")\n",
    "    test_inputs.append(\"Given the event '\"+word+\"', which of the following arrows best represents this event: ↑, ↓, ←, →.\")\n",
    "print(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_ending(model, tokenizer, prompt_endings, hf):\n",
    "    '''\n",
    "    given a list of prompt endings, find the one that maximizes the probability sum over the 7 likert scores\n",
    "\n",
    "    model: the model to use, either a string of a gpt model or a loaded huggingface model object\n",
    "    tokenizer: huggingface tokenizer object\n",
    "    prompt_endings: list of prompt endings to test\n",
    "    hf: whether to use huggingface or gpt\n",
    "    '''\n",
    "  \n",
    "    score_sums=[]                                                            \n",
    "    print(\"Number of inputs used to compute probability sum:\", len(test_inputs))     \n",
    "\n",
    "\n",
    "    output_lists=[]             # list containing the avg. generated likert scores \n",
    "    for ending in prompt_endings:\n",
    "        print(\"Testing Prompt Ending:\", ending)\n",
    "        proba_sum, output_list=log_prob_sum(input_samples=test_inputs, model=model, tokenizer=tokenizer, prompt_ending=ending, hf=hf)\n",
    "        score_sums.append(proba_sum)   \n",
    "        output_lists.append(output_list)\n",
    "\n",
    "    # print results\n",
    "    for i in range(len(prompt_endings)):\n",
    "        print(\"Prompt Ending:\", prompt_endings[i].replace(\"\\n\", \"\\\\n\"))\n",
    "        #print(\"Probability Sum:\", score_sums[i])\n",
    "        print(\"Average Probability:\", round(score_sums[i]/len(test_inputs),3))\n",
    "    #    # compute avg. difference in output_list to the other output_lists\n",
    "    #    for j in range(len(prompt_endings)):\n",
    "    #        if i!=j:\n",
    "    #            print(\"Average score difference to Prompt Ending\", prompt_endings[j].replace(\"\\n\",\"\\\\n\"), \":\", \n",
    "    #                np.mean(np.abs(np.array(output_lists[i])-np.array(output_lists[j]))))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_complete(prompt, prompt_ending, model, tokenizer, sampling, max_len):\n",
    "    '''\n",
    "    Given a prompt, generate a completion with a given model\n",
    "    '''\n",
    "    prompt = prompt + prompt_ending\n",
    "    # Step 1: Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(torch.device(\"cuda:\"+str(gpu_id)))\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    # Step 2: Generate the model input\n",
    "    if sampling:\n",
    "        # TODO make sampling params explicit --> temp, top_k\n",
    "        output = model.generate(input_ids, max_new_tokens=max_len, num_return_sequences=1)\n",
    "    else: \n",
    "        output = model.generate(input_ids, max_new_tokens=max_len, num_return_sequences=1, top_k=1)\n",
    "        \n",
    "    \n",
    "    # Step 3: Decode the generated output to get the answer\n",
    "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Step 4: Print the answer\n",
    "    return generated_answer\n",
    "\n",
    "def hf_complete_proba(prompt, prompt_ending, model, tokenizer, verbose):\n",
    "\n",
    "    start = prompt+prompt_ending\n",
    "\n",
    "    res_ends = []\n",
    "    proba_list = []\n",
    "    for j, end in answers.items():\n",
    "        input_prompt = start+end \n",
    "        if verbose: print(input_prompt)\n",
    "        logprobs = logprobs_from_prompt(input_prompt, tokenizer, model, gpu_id)\n",
    "        res = {\"tokens\": [x for x,y in logprobs],\"token_logprobs\": [y for x,y in logprobs]}\n",
    "        res_ends.append(res)\n",
    "        proba_list.append(np.exp(res[\"token_logprobs\"][-1]))\n",
    "        if verbose:print(end, res, \"\\n\")\n",
    "\n",
    "    average_completion = 0\n",
    "    for i in range(len(proba_list)):\n",
    "        average_completion += (i+1) * proba_list[i]\n",
    "    average_completion = average_completion / sum(proba_list)      \n",
    "        \n",
    "    return proba_list, average_completion, res_ends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the endings to be tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_endings=[\" Arrow:\", \n",
    "                \" A research participant would choose the arrow:\", \n",
    "                \" I choose the arrow:\", \n",
    "                \" Arrow: \", \n",
    "                \" A research participant would choose the arrow: \", \n",
    "                \" I choose the arrow: \",\n",
    "                \"  \",\n",
    "                \" The concept is best represented by arrow \",\n",
    "                \" I would choose arrow \",\n",
    "                \" I choose arrow \",\n",
    "                \" I choose the arrow \",\n",
    "                \" I think it's arrow \",\n",
    "                \" It is the arrow: \",\n",
    "                \" It is arrow: \",\n",
    "                \" I choose the arrow:\\n\",]\n",
    "find_best_ending(model=model, tokenizer=tokenizer, prompt_endings=prompt_endings, hf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Pseudo-Visual` Winner:\n",
    "\n",
    "On average 96.5% of the answers are selected from the arrows as next token.\n",
    "\n",
    "Prompt: `Given the event '`WORD`', which of the following arrows best represents this event: ↑, ↓, ←, →.`\n",
    "\n",
    "Ending: ` Arrow: `\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeating for `TEXTUAL` prompts\n",
    "\n",
    "(restart kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'fled', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'pointed at', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'pulled', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'pushed', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'walked', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'hunted', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'impacted', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'perched', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'showed', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'smashed', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'bombed', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'flew', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'floated', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'lifted', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'sank', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'argued with', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'gave to', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'offended', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'rushed', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'warned', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'owned', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'regretted', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'rested', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'tempted', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'wanted', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'hoped', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'increased', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'obeyed', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'respected', \", \"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event 'succeeded', \"]\n"
     ]
    }
   ],
   "source": [
    "test_inputs = []\n",
    "answers = {0:\"up\", 1:\"down\", 2:\"left\", 3:\"right\"}\n",
    "\n",
    "# 'up', 'down', 'left', 'right'\n",
    "# Prompt Ending:  I choose the concept: \n",
    "# Average Probability: 0.592\n",
    "\n",
    "# 'Up', 'Down', 'Left', 'Right'\n",
    "# Prompt Ending:  I choose the concept: \n",
    "# Average Probability: 0.58\n",
    "\n",
    "# 0:'UP', 1:'DOWN', 2:'LEFT', 3:'RIGHT'\n",
    "# Prompt Ending:  I choose the concept: \n",
    "# Average Probability: 0.636\n",
    "\n",
    "# {0:\"up\", 1:\"down\", 2:\"left\", 3:\"right\"}\n",
    "# Prompt Ending:  I would choose '\n",
    "# Average Probability: 0.806\n",
    "\n",
    "for word in list(action_words):\n",
    "    #test_inputs.append(\"Given the event '\"+word+\"', which of the following concepts best represents this event: up, down, left, right.\")\n",
    "    #test_inputs.append(\"Given the event '\"+word+\"', which of the following concepts best represents this event: Up, Down, Left, Right.\")\n",
    "    #test_inputs.append(\"Given the event '\"+word+\"', which of the following concepts best represents this event: UP, DOWN, LEFT, RIGHT.\")\n",
    "    #test_inputs.append(\"Given the concepts: up, down, left, right. For the concept that best represent the event '\"+word+\"', \")\n",
    "    test_inputs.append(\"Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event '\"+word+\"', \")\n",
    "print(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the concepts: up, down, left, right. For the concept that best represent the event 'jump', I choose 'up'.\n",
      "G\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"Given the concepts: up, down, left, right. For the concept that best represent the event 'jump', I choose\"\n",
    "\n",
    "input_ids = tokenizer.encode(test_prompt, return_tensors=\"pt\").to(torch.device(\"cuda:\"+str(gpu_id)))\n",
    "max_length = input_ids.size(1)  + 5\n",
    "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, top_k=1)\n",
    "generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)  \n",
    "\n",
    "print(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of inputs used to compute probability sum: 30\n",
      "Testing Prompt Ending:  I choose '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Prompt Ending:  I choose: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Prompt Ending:  I would choose '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Prompt Ending:  I would choose: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Prompt Ending:  I choose concept '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Prompt Ending:  I choose concept: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Prompt Ending:  I choose the concept '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Prompt Ending:  I choose the concept: '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:15<00:00,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Ending:  I choose '\n",
      "Average Probability: 0.816\n",
      "\n",
      "Prompt Ending:  I choose: '\n",
      "Average Probability: 0.745\n",
      "\n",
      "Prompt Ending:  I would choose '\n",
      "Average Probability: 0.806\n",
      "\n",
      "Prompt Ending:  I would choose: '\n",
      "Average Probability: 0.733\n",
      "\n",
      "Prompt Ending:  I choose concept '\n",
      "Average Probability: 0.767\n",
      "\n",
      "Prompt Ending:  I choose concept: '\n",
      "Average Probability: 0.708\n",
      "\n",
      "Prompt Ending:  I choose the concept '\n",
      "Average Probability: 0.742\n",
      "\n",
      "Prompt Ending:  I choose the concept: '\n",
      "Average Probability: 0.688\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_endings=[\n",
    "\n",
    "                \" I choose '\",\n",
    "                \" I choose: '\",\n",
    "                \" I would choose '\",\n",
    "                \" I would choose: '\",\n",
    "                \" I choose concept '\",\n",
    "                \" I choose concept: '\",\n",
    "                \" I choose the concept '\",\n",
    "                \" I choose the concept: '\",\n",
    "                ]\n",
    "\n",
    "find_best_ending(model=model, tokenizer=tokenizer, prompt_endings=prompt_endings, hf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Textual` Winner:\n",
    "\n",
    "On average 80.6% of the answers are selected from the concepts as next token.\n",
    "\n",
    "Prompt: `Given the concepts: 'up', 'down', 'left', 'right'. For the concept that best represent the event '`WORD`',` \n",
    "\n",
    "Ending: ` I would choose '`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
