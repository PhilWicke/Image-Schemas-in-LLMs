{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from sidemethods import logprobs_from_prompt, proc, proc_lower, prob_of_ending\n",
    "server_model_path = \"/mounts/data/corp/huggingface/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /mounts/Users/cisintern/pwicke/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "with open(\"../../hf.key\", \"r\") as f_in:\n",
    "    hf_key = f_in.readline().strip()\n",
    "login(token = hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "#model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model_name = \"meta-llama/Llama-2-13b-hf\"\n",
    "#model_name = \"meta-llama/Llama-2-13b-chat-hf\"\n",
    "#model_name = \"meta-llama/Llama-2-70b-hf\"\n",
    "#model_name = \"meta-llama/Llama-2-70b-chat-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2872d91252423e81d4d000ad419e34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the GPU ID you want to use\n",
    "gpu_id = 1\n",
    "\n",
    "# Use the torch.cuda.device() context manager to set the current GPU\n",
    "with torch.cuda.device(gpu_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name) # use_auth_token=True\n",
    "    model = AutoModelForCausalLM.from_pretrained(server_model_path+model_name).to(torch.device(\"cuda\")) # use_auth_token=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob_sum(input_samples, model, tokenizer, prompt_ending, hf):\n",
    "    '''\n",
    "    compute the sum of log probabilities attributed to the 7 allowed scores over x=len(input_samples) samples\n",
    "\n",
    "    input_samples: list of prompts\n",
    "    model: the model to use, either a string of a gpt model or a loaded huggingface model object\n",
    "    tokenizer: huggingface tokenizer object\n",
    "    prompt_ending: the string that is appended to the prompt\n",
    "    hf: whether to use huggingface or gpt\n",
    "    '''\n",
    "    proba_sum = 0\n",
    "    avg_output_list = []    #this contains probability-averaged likert scores per prompt\n",
    "    for prompt in input_samples:\n",
    "        if hf:\n",
    "            proba_list, avg_likert, _ = hf_complete_proba(prompt=prompt, model=model, tokenizer=tokenizer, prompt_ending=prompt_ending, verbose=False)\n",
    "        else:\n",
    "            proba_list, avg_likert, debug = gpt_complete_proba(prompt, model, prompt_ending, verbose=True)\n",
    "        proba_sum += sum(proba_list)\n",
    "        avg_output_list.append(avg_likert)\n",
    "    return proba_sum, avg_output_list       # return sum and the list of averaged likert scores per prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_ending(model, tokenizer, prompt_endings, hf):\n",
    "    '''\n",
    "    given a list of prompt endings, find the one that maximizes the probability sum over the 7 likert scores\n",
    "\n",
    "    model: the model to use, either a string of a gpt model or a loaded huggingface model object\n",
    "    tokenizer: huggingface tokenizer object\n",
    "    prompt_endings: list of prompt endings to test\n",
    "    hf: whether to use huggingface or gpt\n",
    "    '''\n",
    "    test_inputs = []    \n",
    "    score_sums=[]                                                          \n",
    "    print(test_inputs)    \n",
    "    # [\n",
    "    # 'Consider the notion of VERTICALITY. Verticality refers to the sense of an extension along an up—down orientation.\\n\n",
    "    # How strongly is the phrase \"stand at attention\" related to this notion on a scale from 1 (not at all related) to 7\n",
    "    # (very strongly related)?', \n",
    "    \n",
    "    # 'Consider the notion of VERTICALITY. Verticality refers to the sense of an extension along \n",
    "    # an up—down orientation.\\nHow strongly is the phrase \"stand out in several sports\" related to this notion on a scale \n",
    "    # from 1 (not at all related) to 7 (very strongly related)?', \n",
    "\n",
    "    # 'Consider the notion of VERTICALITY. Verticality refers to the sense of an extension along an up—down orientation.\\n\n",
    "    # How strongly is the phrase \"to stand firm\" related to this notion on a scale from 1 (not at all related) to 7 (very \n",
    "    # strongly related)?', \n",
    "\n",
    "    # 'Consider the notion of BALANCE. Balance \n",
    "    # refers to your sense of symmetry or stability relative to some point within your body.\\nHow strongly is the phrase \n",
    "    # \"stand at attention\" related to this notion on a scale from 1 (not at all related) to 7 (very strongly related)?', \n",
    "\n",
    "    # 'Consider the notion of BALANCE. Balance refers to your sense of symmetry or stability relative to some point within \n",
    "    # your body.\\nHow strongly is the phrase \"stand out in several sports\" related to this notion on a scale from 1 (not at \n",
    "    # all related) to 7 (very strongly related)?', \n",
    "\n",
    "    # 'Consider the notion of BALANCE. Balance refers to your sense of symmetry \n",
    "    # or stability relative to some point within your body.\\nHow strongly is the phrase \"to stand firm\" related to this notion \n",
    "    # on a scale from 1 (not at all related) to 7 (very strongly related)?', \n",
    "    print(\"Number of inputs used to compute probability sum:\", len(test_inputs))     \n",
    "\n",
    "\n",
    "    output_lists=[]             # list containing the avg. generated likert scores \n",
    "    for ending in prompt_endings:\n",
    "        print(\"Testing Prompt Ending:\", ending)\n",
    "        proba_sum, output_list=log_prob_sum(input_samples=test_inputs, model=model, tokenizer=tokenizer, prompt_ending=ending, hf=hf)\n",
    "        score_sums.append(proba_sum)   \n",
    "        output_lists.append(output_list)\n",
    "\n",
    "    # print results\n",
    "    for i in range(len(prompt_endings)):\n",
    "        print(\"Prompt Ending:\", prompt_endings[i].replace(\"\\n\", \"\\\\n\"))\n",
    "        print(\"Probability Sum:\", score_sums[i])\n",
    "        print(\"Average Probability:\", score_sums[i]/len(test_inputs))\n",
    "        # compute avg. difference in output_list to the other output_lists\n",
    "        for j in range(len(prompt_endings)):\n",
    "            if i!=j:\n",
    "                print(\"Average score difference to Prompt Ending\", prompt_endings[j].replace(\"\\n\",\"\\\\n\"), \":\", \n",
    "                    np.mean(np.abs(np.array(output_lists[i])-np.array(output_lists[j]))))\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hf_complete(prompt, prompt_ending, model, tokenizer, sampling, max_len):\n",
    "    '''\n",
    "    Given a prompt, generate a completion with a given model\n",
    "    '''\n",
    "    prompt = prompt + prompt_ending\n",
    "    # Step 1: Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to(model.device)\n",
    "    \n",
    "    # Step 2: Generate the model input\n",
    "    if sampling:\n",
    "        # TODO make sampling params explicit --> temp, top_k\n",
    "        output = model.generate(input_ids, max_new_tokens=max_len, num_return_sequences=1)\n",
    "    else: \n",
    "        output = model.generate(input_ids, max_new_tokens=max_len, num_return_sequences=1, top_k=1)\n",
    "        \n",
    "    \n",
    "    # Step 3: Decode the generated output to get the answer\n",
    "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Step 4: Print the answer\n",
    "    return generated_answer\n",
    "\n",
    "def hf_complete_proba(prompt, prompt_ending, model, tokenizer, verbose):\n",
    "    answers = {0:\"1\", 1:\"2\", 2:\"3\", 3:\"4\", 4:\"5\", 5:\"6\", 6:\"7\"}\n",
    "\n",
    "    start = prompt+prompt_ending\n",
    "\n",
    "    res_ends = []\n",
    "    proba_list = []\n",
    "    for j, end in answers.items():\n",
    "        input_prompt = start+end \n",
    "        if verbose: print(input_prompt)\n",
    "        logprobs = logprobs_from_prompt(input_prompt, tokenizer, model)\n",
    "        res = {\"tokens\": [x for x,y in logprobs],\"token_logprobs\": [y for x,y in logprobs]}\n",
    "        res_ends.append(res)\n",
    "        proba_list.append(np.exp(res[\"token_logprobs\"][-1]))\n",
    "        if verbose:print(end, res, \"\\n\")\n",
    "\n",
    "    average_completion = 0\n",
    "    for i in range(len(proba_list)):\n",
    "        average_completion += (i+1) * proba_list[i]\n",
    "    average_completion = average_completion / sum(proba_list)      \n",
    "        \n",
    "    return proba_list, average_completion, res_ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_endings=[\" I choose the arrow\", \" A research participant would choose the arrow \", \" I choose the arrow \"]\n",
    "find_best_ending(model=model, tokenizer=tokenizer, prompt_endings=prompt_endings, hf=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
