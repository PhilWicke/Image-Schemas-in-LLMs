{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 01\n",
    "### Richardson et. al (2002) Experiment 01 with Language Models instead of Humans:\n",
    "\n",
    "The subjects were presented with a single page,\n",
    "containing a list of the verbs and four pictures, labelled A to\n",
    "D. Each one contained a circle and a square aligned along a\n",
    "vertical or horizontal axis, connected by an arrow pointing\n",
    "up, down, left or right. Since we didn't expect any\n",
    "interesting item variation between left or right placement of\n",
    "the circle or square, the horizontal schemas differed only in\n",
    "the direction of the arrow.\n",
    "For each sentence, subjects were asked to select one of\n",
    "the four sparse images that best depicted the event described\n",
    "by the sentence (Figure 1)\n",
    "The items were randomised in three different orders, and\n",
    "crossed with two different orderings of the images. The six\n",
    "lists were then distributed randomly to subjects.\n",
    "\n",
    "### Step 01: Creating a prompt that is as close to the paper as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fled': [7.2, 4.2, 80.8, 7.8], 'pointed at': [7.2, 3.6, 0.0, 89.2], 'pulled': [6.0, 5.4, 75.4, 13.2], 'pushed': [7.2, 3.6, 1.2, 8.0, 8.0], 'walked': [9.0, 3.6, 2.0, 4.0, 62.9], 'hunted': [9.6, 20.4, 1.8, 68.3], 'impacted': [7.2, 37.1, 3.0, 52.7], 'perched': [1.0, 2.0, 7.0, 6.0, 6.6, 5.4], 'showed': [1.0, 5.0, 9.0, 10.2, 65.9], 'smashed': [3.6, 66.5, 1.2, 28.7], 'bombed': [4.8, 86.8, 1.8, 6.6], 'flew': [37.7, 44.3, 15.0, 3.0], 'floated': [32.9, 56.3, 7.8, 3.0], 'lifted': [87.4, 9.6, 2.4, 0.6], 'sank': [22.2, 71.9, 4.2, 1.8], 'argued with': [11.4, 13.8, 12.6, 62.3], 'gave to': [8.4, 9.6, 1.2, 80.8], 'offended': [9.0, 31.7, 24.6, 34.7], 'rushed': [10.2, 10.8, 23.4, 55.1], 'warned': [10.8, 22.2, 6.0, 61.1], 'owned': [5.4, 55.7, 18.6, 20.4], 'regretted': [19.8, 24.0, 41.3, 1.0, 5.0], 'rested': [14.4, 36.5, 40.1, 9.0], 'tempted': [16.8, 11.4, 45.5, 26.3], 'wanted': [15.6, 7.8, 15.6, 61.1], 'hoped': [45.5, 15.6, 7.2, 31.7], 'increased': [73.7, 7.2, 9.6, 9.0], 'obeyed': [22.8, 4.2, 64.7, 8.4], 'respected': [53.9, 3.0, 14.4, 28.7], 'succeeded': [40.1, 35.9, 10.8, 13.2], 0: ['C:◯↑▢', 'D:◯↓▢', 'B:◯←▢ ', 'A:◯→▢']}\n",
      "1. ◯ hunted ▢ \\\n",
      "2. ◯ showed ▢ \\\n",
      "3. ◯ succeeded ▢ \\\n",
      "4. ◯ pointed at ▢ \\\n",
      "5. ◯ pushed ▢ \\\n",
      "6. ◯ hoped ▢ \\\n",
      "7. ◯ walked ▢ \\\n",
      "8. ◯ gave to ▢ \\\n",
      "9. ◯ respected ▢ \\\n",
      "10. ◯ smashed ▢ \\\n",
      "11. ◯ argued with ▢ \\\n",
      "12. ◯ sank ▢ \\\n",
      "13. ◯ rested ▢ \\\n",
      "14. ◯ offended ▢ \\\n",
      "15. ◯ pulled ▢ \\\n",
      "16. ◯ perched ▢ \\\n",
      "17. ◯ regretted ▢ \\\n",
      "18. ◯ bombed ▢ \\\n",
      "19. ◯ obeyed ▢ \\\n",
      "20. ◯ lifted ▢ \\\n",
      "21. ◯ flew ▢ \\\n",
      "22. ◯ impacted ▢ \\\n",
      "23. ◯ wanted ▢ \\\n",
      "24. ◯ increased ▢ \\\n",
      "25. ◯ warned ▢ \\\n",
      "26. ◯ floated ▢ \\\n",
      "27. ◯ tempted ▢ \\\n",
      "28. ◯ rushed ▢ \\\n",
      "29. ◯ owned ▢ \\\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(1337)\n",
    "\n",
    "\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return value\n",
    "\n",
    "with open(\"../../data/richardson_actions.txt\", \"r\") as d_in:\n",
    "    lines = [line.split() for line in d_in.readlines()]\n",
    "\n",
    "output = []\n",
    "for entry in lines:\n",
    "    new_entry = [convert_to_float(item) for item in entry]\n",
    "    if isinstance(new_entry[1],str):\n",
    "        new_entry[0] = \" \".join(new_entry[:2])\n",
    "        del new_entry[1]\n",
    "    output.append(new_entry)\n",
    "\n",
    "richardson_data = dict()\n",
    "for elem in output:\n",
    "    richardson_data[elem[0]] = [i for i in elem[1:]]\n",
    "\n",
    "richardson_data[0] = [\"C:◯↑▢\", \"D:◯↓▢\",\"B:◯←▢ \",\"A:◯→▢\"]\n",
    "\n",
    "# Randomizing Richardson's data\n",
    "action_words = list(richardson_data.keys())\n",
    "del action_words[0]\n",
    "random.shuffle(action_words)\n",
    "\n",
    "#count=1\n",
    "#for line in action_words:\n",
    "#    if line != 0:\n",
    "#        print(str(count)+\". ◯ \"+line+\" ▢ \\\\\")\n",
    "#        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_prompt = \"You are asked to select one of the four images that best depicts the event described by the following sentences. \\\n",
    "Image A: \\\n",
    "◯→▢ \\\n",
    "\\\n",
    "Image B: \\\n",
    "◯←▢ \\\n",
    " \\\n",
    "Image C: \\\n",
    "◯ \\\n",
    "↑ \\\n",
    "▢ \\\n",
    "\\\n",
    "Image D: \\\n",
    "◯ \\\n",
    "↓ \\\n",
    "▢ \\\n",
    "\\\n",
    "Sentences: \\\n",
    "1. ◯ hunted ▢ \\\n",
    "2. ◯ showed ▢ \\\n",
    "3. ◯ succeeded ▢ \\\n",
    "4. ◯ pointed at ▢ \\\n",
    "5. ◯ pushed ▢ \\\n",
    "6. ◯ hoped ▢ \\\n",
    "7. ◯ walked ▢ \\\n",
    "8. ◯ gave to ▢ \\\n",
    "9. ◯ respected ▢ \\\n",
    "10. ◯ smashed ▢ \\\n",
    "11. ◯ argued with ▢ \\\n",
    "12. ◯ sank ▢ \\\n",
    "13. ◯ rested ▢ \\\n",
    "14. ◯ offended ▢ \\\n",
    "15. ◯ pulled ▢ \\\n",
    "16. ◯ perched ▢ \\\n",
    "17. ◯ regretted ▢ \\\n",
    "18. ◯ bombed ▢ \\\n",
    "19. ◯ obeyed ▢ \\\n",
    "20. ◯ lifted ▢ \\\n",
    "21. ◯ flew ▢ \\\n",
    "22. ◯ impacted ▢ \\\n",
    "23. ◯ wanted ▢ \\\n",
    "24. ◯ increased ▢ \\\n",
    "25. ◯ warned ▢ \\\n",
    "26. ◯ floated ▢ \\\n",
    "27. ◯ tempted ▢ \\\n",
    "28. ◯ rushed ▢ \\\n",
    "29. ◯ owned ▢ \\\n",
    "\\\n",
    "For sentence 1) choosing from [A,B,C,D] best image is \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 02: Test model with this `paper prompt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to evaluate against Richardson's findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /mounts/data/corp/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, OPTForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast, GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "\n",
    "with open(\"../../hf.key\", \"r\") as f_in:\n",
    "    hf_key = f_in.readline().strip()\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", hf_key])\n",
    "\n",
    "server_model_path = \"/mounts/data/corp/huggingface/\"\n",
    "# stored using: git clone git@hf.co:<MODEL ID>\n",
    "\n",
    "model_prefix = \"EleutherAI/gpt-neox\"\n",
    "model_size = \"20b\"\n",
    "\n",
    "model_prefix = \"facebook/opt\"\n",
    "model_size = \"13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2857f84f86794c3b887b4944a85acf80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenizer = GPTNeoXTokenizerFast.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "#model = GPTNeoXForCausalLM.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "#model = OPTForCausalLM.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\", use_auth_token=True, device_map=\"auto\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(server_model_path+\"llama/llama-13b\", use_auth_token=True, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-70b-hf\", use_auth_token=True, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(server_model_path+\"meta-llama/Llama-2-70b-hf\", use_auth_token=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenize the prompt\n",
    "input_ids = tokenizer.encode(paper_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Step 2: Generate the model input\n",
    "max_length = input_ids.size(1)  + 300 # Adjust '20' as needed to control the maximum length of the generated answer.\n",
    "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "# Step 3: Decode the generated output to get the answer\n",
    "generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer:\n",
      "\n",
      " ◯→▢. For sentence 2) choosing from [A,B,C,D] best image is ◯←▢. For sentence 3) choosing from [A,B,C,D] best image is ◯←▢. For sentence 4) choosing from [A,B,C,D] best image is ◯←▢. For sentence 5) choosing from [A,B,C,D] best image is ◯←▢. For sentence 6) choosing from [A,B,C,D] best image is ◯←▢. For sentence 7) choosing from [A,B,C,D] best image is ◯←▢. For sentence 8) choosing from [A,B,C,D] best image is ◯←▢. For sentence 9) choosing from [A,B,C,D] best image is ◯←▢. For sentence 10) choosing from [A,B,C,D] best image is ◯←▢. For sentence 11) choosing from [A,B,C,D] best image is ◯←▢. For sentence 12) choosing from [A\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated answer:\\n\\n\", generated_answer[len(paper_prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 03: Prompting that is maximally friendly for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION: hunted\n",
      "Generated answer:\n",
      "\n",
      " →▢].\n",
      "\n",
      "### 2.\n",
      "\n",
      "Select the image that best represents\n",
      "ACTION: showed\n",
      "Generated answer:\n",
      "\n",
      " ←▢].\n",
      "\n",
      "### 2.\n",
      "\n",
      "Select the image that best represents\n",
      "ACTION: succeeded\n",
      "Generated answer:\n",
      "\n",
      " →▢].\n",
      "\n",
      "### 2.\n",
      "\n",
      "Select the image that best represents\n",
      "ACTION: pointed at\n",
      "Generated answer:\n",
      "\n",
      " ←▢].\n",
      "\n",
      "Select the image that best represents the event described by the sentence:\n",
      "ACTION: pushed\n",
      "Generated answer:\n",
      "\n",
      " ←▢].\n",
      "\n",
      "Select the image that best represents the event described by the sentence:\n",
      "ACTION: hoped\n",
      "Generated answer:\n",
      "\n",
      " \n",
      "↓\n",
      "▢].\n",
      "\n",
      "Select the image that best represents the event described by the\n",
      "ACTION: walked\n",
      "Generated answer:\n",
      "\n",
      " →▢].\n",
      "\n",
      "### 2.\n",
      "\n",
      "Select the image that best represents\n",
      "ACTION: gave to\n",
      "Generated answer:\n",
      "\n",
      " ←▢].\n",
      "\n",
      "Select the image that best represents the event described by the sentence:\n",
      "ACTION: respected\n",
      "Generated answer:\n",
      "\n",
      " \n",
      "↓\n",
      "▢].\n",
      "\n",
      "Select the image that best represents the event described by the\n",
      "ACTION: smashed\n",
      "Generated answer:\n",
      "\n",
      " →▢].\n",
      "\n",
      "Select the image that best represents the event described by the sentence:\n",
      "ACTION: argued with\n",
      "Generated answer:\n",
      "\n",
      " ←▢].\n",
      "\n",
      "### 2.\n",
      "\n",
      "Select the image that best represents\n",
      "ACTION: sank\n",
      "Generated answer:\n",
      "\n",
      " \n",
      "↓\n",
      "▢].\n",
      "\n",
      "Select the image that best represents the event described by the\n",
      "ACTION: rested\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m input_ids \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(friendly_prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m max_length \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)  \u001b[39m+\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m----> 9\u001b[0m output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(input_ids, max_length\u001b[39m=\u001b[39;49mmax_length, num_return_sequences\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     10\u001b[0m generated_answer \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \n\u001b[1;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGenerated answer:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m, generated_answer[\u001b[39mlen\u001b[39m(friendly_prompt):])\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/generation/utils.py:1538\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1532\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1533\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnum_return_sequences has to be 1 when doing greedy search, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1534\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut is \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1535\u001b[0m         )\n\u001b[1;32m   1537\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1538\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1539\u001b[0m         input_ids,\n\u001b[1;32m   1540\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1541\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1542\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1543\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1544\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1545\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1546\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1547\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1548\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1549\u001b[0m     )\n\u001b[1;32m   1551\u001b[0m \u001b[39melif\u001b[39;00m is_contrastive_search_gen_mode:\n\u001b[1;32m   1552\u001b[0m     \u001b[39mif\u001b[39;00m generation_config\u001b[39m.\u001b[39mnum_return_sequences \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/generation/utils.py:2362\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2359\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2361\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2362\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2363\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2364\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2365\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2366\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2367\u001b[0m )\n\u001b[1;32m   2369\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2370\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39;49m_hf_hook\u001b[39m.\u001b[39;49mpost_forward(module, output)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/accelerate/hooks.py:296\u001b[0m, in \u001b[0;36mAlignDevicesHook.post_forward\u001b[0;34m(self, module, output)\u001b[0m\n\u001b[1;32m    293\u001b[0m         set_module_tensor_to_device(module, name, \u001b[39m\"\u001b[39m\u001b[39mmeta\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    295\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mio_same_device \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_device \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 296\u001b[0m     output \u001b[39m=\u001b[39m send_to_device(output, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_device, skip_keys\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mskip_keys)\n\u001b[1;32m    298\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/accelerate/utils/operations.py:161\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 161\u001b[0m         {\n\u001b[1;32m    162\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39mnon_blocking, skip_keys\u001b[39m=\u001b[39mskip_keys)\n\u001b[1;32m    163\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    164\u001b[0m         }\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    167\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/accelerate/utils/operations.py:162\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    160\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[1;32m    161\u001b[0m         {\n\u001b[0;32m--> 162\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[1;32m    163\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    164\u001b[0m         }\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    167\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/accelerate/utils/operations.py:168\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    167\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 168\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mto(device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking)\n\u001b[1;32m    169\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    170\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for action_word in action_words:\n",
    "    \n",
    "    print(\"ACTION: \"+action_word)\n",
    "\n",
    "    friendly_prompt = \"Select the image that best represents the event described by the sentence: \"+action_word+\"\\n[◯→▢]\\n\\n[◯←▢]\\n\\n[◯\\n↑\\n▢]\\n\\n[◯\\n↓\\n▢]\\n\\nThe best representation is [◯\"\n",
    "    \n",
    "    input_ids = tokenizer.encode(friendly_prompt, return_tensors=\"pt\")\n",
    "    max_length = input_ids.size(1)  + 20\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)  \n",
    "    print(\"Generated answer:\\n\\n\", generated_answer[len(friendly_prompt):])\n",
    "\n",
    "prompt_B = \"Select the image that best represents the event described by the sentence: \"+action_word+\"\\n[◯→▢]\\n\\n[◯←▢]\\n\\n[◯\\n↑\\n▢]\\n\\n[◯\\n↓\\n▢]\\n\\nThe best representation is [◯\"\n",
    "#prompt_C = \"Choose the best image for the word:\\nUP: ↑ \\nDOWN: ↓ \\nLEFT: → \\nRIGHT: ← \\n\"+action_word.upper()+\": \"\n",
    "#prompt_D = \"Choosing from UP, DOWN, LEFT and RIGHT, the word \\'\"+action_word.upper()+\"\\' is best respresented by the word \"\n",
    "#prompt_B = \"Of these four: A: ◯→▢ B: ◯←▢ C: ◯ ↑ ▢ D: ◯ ↓ ▢ Which one best describes \\\"◯ \"+action_word+\" ▢\\\" ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hunted\n",
      "showed\n",
      "succeeded\n",
      "pointed at\n",
      "pushed\n",
      "hoped\n",
      "walked\n",
      "gave to\n",
      "respected\n",
      "smashed\n",
      "argued with\n",
      "sank\n",
      "rested\n",
      "offended\n",
      "pulled\n",
      "0\n",
      "perched\n",
      "regretted\n",
      "bombed\n",
      "obeyed\n",
      "lifted\n",
      "flew\n",
      "impacted\n",
      "wanted\n",
      "increased\n",
      "warned\n",
      "floated\n",
      "tempted\n",
      "rushed\n",
      "owned\n"
     ]
    }
   ],
   "source": [
    "for action in action_words:\n",
    "    print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 03: Test pipline with logprobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sidemethods import logprobs_from_prompt, proc, proc_lower, prob_of_ending, calculate_accuracy, calculate_accuracies, store_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start = friendly_prompt\n",
    "#start = prompt_B\n",
    "#start = prompt_D\n",
    "\n",
    "#answers = {0:\"A ◯→▢\", 1:\"B ◯←▢\", 2:\"C ◯\\n↑\\n▢\", 3:\"D ◯\\n↓\\n▢\"}\n",
    "answers = {0:\"→▢]\", 1:\"←▢]\", 2:\"\\n↑\\n▢]\", 3:\"\\n↓\\n▢]\"}\n",
    "#answers = {0:\"UP\", 1:\"DOWN\", 2:\"LEFT\", 3:\"RIGHT\"}\n",
    "\n",
    "#start = \"nazis are known to be on the political \"\n",
    "#answers = {0:\"UP\", 1:\"DOWN\", 2:\"LEFT\", 3:\"RIGHT\"}\n",
    "\n",
    "\n",
    "res_ends = []\n",
    "for j, end in answers.items():\n",
    "    input_prompt = proc(start) + ' ' + proc(end)\n",
    "    logprobs = logprobs_from_prompt(input_prompt, tokenizer, model)\n",
    "    res = {\"tokens\": [x for x,y in logprobs],\"token_logprobs\": [y for x,y in logprobs]}\n",
    "    res_ends.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select the image that best represents the event described by the sentence: rested\n",
      "[◯→▢]\n",
      "\n",
      "[◯←▢]\n",
      "\n",
      "[◯\n",
      "↑\n",
      "▢]\n",
      "\n",
      "[◯\n",
      "↓\n",
      "▢]\n",
      "\n",
      "The best representation is [◯\n",
      "Choice:  \n",
      "↓\n",
      "▢]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "choosen_answer = (-9999, \"\")\n",
    "for i, answer in answers.items():\n",
    "    choice_val = prob_of_ending(res_ends[i]['token_logprobs'], res_ends[0]['tokens'])\n",
    "    if choice_val > choosen_answer[0]:\n",
    "        choosen_answer = choice_val, answer\n",
    "\n",
    "\n",
    "print(start)\n",
    "print(\"Choice: \", choosen_answer[1])\n",
    "print()\n",
    "\n",
    "#input_ids = tokenizer.encode(start, return_tensors=\"pt\")\n",
    "#input_ids = input_ids.to('cuda')\n",
    "#output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "#generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "#print(\"Generated answer:\", generated_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
