{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 01\n",
    "### Richardson et. al (2002) Experiment 01 with Language Models instead of Humans:\n",
    "\n",
    "The subjects were presented with a single page,\n",
    "containing a list of the verbs and four pictures, labelled A to\n",
    "D. Each one contained a circle and a square aligned along a\n",
    "vertical or horizontal axis, connected by an arrow pointing\n",
    "up, down, left or right. Since we didn't expect any\n",
    "interesting item variation between left or right placement of\n",
    "the circle or square, the horizontal schemas differed only in\n",
    "the direction of the arrow.\n",
    "For each sentence, subjects were asked to select one of\n",
    "the four sparse images that best depicted the event described\n",
    "by the sentence (Figure 1)\n",
    "The items were randomised in three different orders, and\n",
    "crossed with two different orderings of the images. The six\n",
    "lists were then distributed randomly to subjects.\n",
    "\n",
    "### Step 01: Creating a prompt to model the experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_word = \"bombed\"\n",
    "\n",
    "prompt = \"You are asked to select one of the four images that best depicts the event described by the following sentence. \\\n",
    "Image A: \\\n",
    "◯→▢ \\\n",
    "\\\n",
    "Image B: \\\n",
    "◯←▢ \\\n",
    " \\\n",
    "Image C: \\\n",
    "◯ \\\n",
    "↑ \\\n",
    "▢ \\\n",
    "\\\n",
    "Image D: \\\n",
    "◯ \\\n",
    "↓ \\\n",
    "▢\\\n",
    "\\\n",
    "Sentence: \\\n",
    "◯ \"+action_word+\" ▢ \\\n",
    "\\\n",
    "The image that best describes the sentence is image \"\n",
    "\n",
    "prompt_A = \"Of these four images: \\\n",
    "Image A: \\\n",
    "◯→▢ \\\n",
    "\\\n",
    "Image B: \\\n",
    "◯←▢ \\\n",
    " \\\n",
    "Image C: \\\n",
    "◯ \\\n",
    "↑ \\\n",
    "▢ \\\n",
    "\\\n",
    "Image D: \\\n",
    "◯ \\\n",
    "↓ \\\n",
    "▢\\\n",
    "\\\n",
    "The one that best describes \\\"◯ \"+action_word+\" ▢\\\" is image \"\n",
    "\n",
    "\n",
    "prompt_B = \"Select the image that best represents the event described by the sentence: \"+action_word+\"\\n[◯→▢]\\n\\n[◯←▢]\\n\\n[◯\\n↑\\n▢]\\n\\n[◯\\n↓\\n▢]\\n\\nThe best representation is [◯\"\n",
    "\n",
    "prompt_C = \"Choose the best image for the word:\\nUP: ↑ \\nDOWN: ↓ \\nLEFT: → \\nRIGHT: ← \\n\"+action_word.upper()+\": \"\n",
    "prompt_D = \"Choosing from UP, DOWN, LEFT and RIGHT, the word \\'\"+action_word.upper()+\"\\' is best respresented by the word \"\n",
    "\n",
    "\n",
    "\n",
    "# Prompt with mask token for MLM objective \n",
    "mask_token = \"<mask>\"\n",
    "prompt_mlm = prompt+mask_token+\" \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 02: Test pipeline for CausalLM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /mounts/data/corp/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, OPTForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast, GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "\n",
    "with open(\"../../hf.key\", \"r\") as f_in:\n",
    "    hf_key = f_in.readline().strip()\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", hf_key])\n",
    "\n",
    "model_prefix = \"EleutherAI/gpt-neox\"\n",
    "model_size = \"20b\"\n",
    "\n",
    "model_prefix = \"facebook/opt\"\n",
    "model_size = \"13b\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366444e12c644c4c821b52de8c4b5de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1521ed6645c4209b353a790cf21de47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999b148709ec4bb9944853a0b1cdf68d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d30097c95d4ba5a1aa756fd29bb5c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dfdc16e1057413d9a5a04b4821b7df3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/610 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "104238f5c5044befb24204f28529490c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)model.bin.index.json:   0%|          | 0.00/33.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89e1b0c0ad142b89b99a88dfa5f6c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed9e7407b29f44a483cc3dfcdc9cf597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00001-of-00003.bin:   0%|          | 0.00/9.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb2783c96cc4c8282ee38413e633cbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00002-of-00003.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd05c062dc714e4eb3c42d1440bae87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)l-00003-of-00003.bin:   0%|          | 0.00/6.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4616429e354c07a0feed281a5a1e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45be8242db8c4cb9829625cddebe6d58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/167 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenizer = GPTNeoXTokenizerFast.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "#model = GPTNeoXForCausalLM.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "#model = OPTForCausalLM.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\", use_auth_token=True, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-hf\", use_auth_token=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/generation/utils.py:1405: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenize the prompt\n",
    "input_ids = tokenizer.encode(prompt_B, return_tensors=\"pt\")\n",
    "\n",
    "# Step 2: Generate the model input\n",
    "max_length = input_ids.size(1) + 30  # Adjust '20' as needed to control the maximum length of the generated answer.\n",
    "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "# Step 3: Decode the generated output to get the answer\n",
    "generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer:\n",
      "\n",
      " Select the image that best represents the event described by the sentence: bombed\n",
      "[◯→▢]\n",
      "\n",
      "[◯←▢]\n",
      "\n",
      "[◯\n",
      "↑\n",
      "▢]\n",
      "\n",
      "[◯\n",
      "↓\n",
      "▢]\n",
      "\n",
      "The best representation is [◯\n",
      "↓\n",
      "▢].\n",
      "\n",
      "### Explanation\n",
      "\n",
      "The image that best represents the event described by the sentence is a bomb\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated answer:\\n\\n\", generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer: Answer the following question: What is the captial of Australia? The capital of Australia is Canberra.\n",
      "What is the capital of Australia? The capital of Australia is Canberra\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenize the prompt\n",
    "input_ids = tokenizer.encode(\"Answer the following question: What is the captial of Australia? The capital of Australia is\", return_tensors=\"pt\")\n",
    "\n",
    "# Step 2: Generate the model input\n",
    "max_length = input_ids.size(1) + 20  # Adjust '20' as needed to control the maximum length of the generated answer.\n",
    "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "# Step 3: Decode the generated output to get the answer\n",
    "generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated answer:\", generated_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 03: Test pipline with logprobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sidemethods import logprobs_from_prompt, proc, proc_lower, prob_of_ending, calculate_accuracy, calculate_accuracies, store_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#start = prompt\n",
    "#start = prompt_B\n",
    "#start = prompt_D\n",
    "\n",
    "#answers = {0:\"A ◯→▢\", 1:\"B ◯←▢\", 2:\"C ◯\\n↑\\n▢\", 3:\"D ◯\\n↓\\n▢\"}\n",
    "#answers = {0:\"→▢]\", 1:\"←▢]\", 2:\"\\n↑\\n▢]\", 3:\"\\n↓\\n▢]\"}\n",
    "answers = {0:\"UP\", 1:\"DOWN\", 2:\"LEFT\", 3:\"RIGHT\"}\n",
    "\n",
    "start = \"nazis are known to be on the political \"\n",
    "answers = {0:\"UP\", 1:\"DOWN\", 2:\"LEFT\", 3:\"RIGHT\"}\n",
    "\n",
    "\n",
    "res_ends = []\n",
    "for j, end in answers.items():\n",
    "    input_prompt = proc(start) + ' ' + proc(end)\n",
    "    logprobs = logprobs_from_prompt(input_prompt, tokenizer, model)\n",
    "    res = {\"tokens\": [x for x,y in logprobs],\"token_logprobs\": [y for x,y in logprobs]}\n",
    "    res_ends.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nazis are known to be on the political \n",
      "Choice:  RIGHT\n",
      "\n",
      "Generated answer: nazis are known to be on the political  right.\n",
      "> > > > > > > > > > > > > > > > > > > > > > > > >\n"
     ]
    }
   ],
   "source": [
    "choosen_answer = (-9999, \"\")\n",
    "for i, answer in answers.items():\n",
    "    choice_val = prob_of_ending(res_ends[i]['token_logprobs'], res_ends[0]['tokens'])\n",
    "    if choice_val > choosen_answer[0]:\n",
    "        choosen_answer = choice_val, answer\n",
    "\n",
    "\n",
    "print(start)\n",
    "print(\"Choice: \", choosen_answer[1])\n",
    "print()\n",
    "\n",
    "input_ids = tokenizer.encode(start, return_tensors=\"pt\")\n",
    "input_ids = input_ids.to('cuda')\n",
    "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated answer:\", generated_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
