{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 01\n",
    "### Richardson et. al (2002) Experiment 01 with Language Models instead of Humans:\n",
    "\n",
    "The subjects were presented with a single page,\n",
    "containing a list of the verbs and four pictures, labelled A to\n",
    "D. Each one contained a circle and a square aligned along a\n",
    "vertical or horizontal axis, connected by an arrow pointing\n",
    "up, down, left or right. Since we didn't expect any\n",
    "interesting item variation between left or right placement of\n",
    "the circle or square, the horizontal schemas differed only in\n",
    "the direction of the arrow.\n",
    "For each sentence, subjects were asked to select one of\n",
    "the four sparse images that best depicted the event described\n",
    "by the sentence (Figure 1)\n",
    "The items were randomised in three different orders, and\n",
    "crossed with two different orderings of the images. The six\n",
    "lists were then distributed randomly to subjects.\n",
    "\n",
    "### Step 01: Creating a prompt that is as close to the paper as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1337)\n",
    "\n",
    "\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return value\n",
    "\n",
    "with open(\"../../data/richardson_actions.txt\", \"r\") as d_in:\n",
    "    lines = [line.split() for line in d_in.readlines()]\n",
    "\n",
    "output = []\n",
    "for entry in lines:\n",
    "    new_entry = [convert_to_float(item) for item in entry]\n",
    "    \n",
    "    if isinstance(new_entry[1],str):\n",
    "        new_entry[0] = \" \".join(new_entry[:2])\n",
    "        del new_entry[1]\n",
    "    output.append(new_entry)\n",
    "\n",
    "richardson_data = dict()\n",
    "for elem in output:\n",
    "    richardson_data[elem[0]] = [i for i in elem[1:]]\n",
    "\n",
    "richardson_data[0] = [\"C:◯↑▢\", \"D:◯↓▢\",\"B:◯←▢ \",\"A:◯→▢\"]\n",
    "\n",
    "# Randomizing Richardson's data\n",
    "action_words = list(richardson_data.keys())\n",
    "random.shuffle(action_words)\n",
    "\n",
    "#count=1\n",
    "#for line in action_words:\n",
    "#    if line != 0:\n",
    "#        print(str(count)+\". ◯ \"+line+\" ▢ \\\\\")\n",
    "#        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_prompt = \"You are asked to select one of the four images that best depicts the event described by the following sentences. \\\n",
    "Image A: \\\n",
    "◯→▢ \\\n",
    "\\\n",
    "Image B: \\\n",
    "◯←▢ \\\n",
    " \\\n",
    "Image C: \\\n",
    "◯ \\\n",
    "↑ \\\n",
    "▢ \\\n",
    "\\\n",
    "Image D: \\\n",
    "◯ \\\n",
    "↓ \\\n",
    "▢ \\\n",
    "\\\n",
    "Sentences: \\\n",
    "1. ◯ hunted ▢ \\\n",
    "2. ◯ showed ▢ \\\n",
    "3. ◯ succeeded ▢ \\\n",
    "4. ◯ pointed at ▢ \\\n",
    "5. ◯ pushed ▢ \\\n",
    "6. ◯ hoped ▢ \\\n",
    "7. ◯ walked ▢ \\\n",
    "8. ◯ gave to ▢ \\\n",
    "9. ◯ respected ▢ \\\n",
    "10. ◯ smashed ▢ \\\n",
    "11. ◯ argued with ▢ \\\n",
    "12. ◯ sank ▢ \\\n",
    "13. ◯ rested ▢ \\\n",
    "14. ◯ offended ▢ \\\n",
    "15. ◯ pulled ▢ \\\n",
    "16. ◯ perched ▢ \\\n",
    "17. ◯ regretted ▢ \\\n",
    "18. ◯ bombed ▢ \\\n",
    "19. ◯ obeyed ▢ \\\n",
    "20. ◯ lifted ▢ \\\n",
    "21. ◯ flew ▢ \\\n",
    "22. ◯ impacted ▢ \\\n",
    "23. ◯ wanted ▢ \\\n",
    "24. ◯ increased ▢ \\\n",
    "25. ◯ warned ▢ \\\n",
    "26. ◯ floated ▢ \\\n",
    "27. ◯ tempted ▢ \\\n",
    "28. ◯ rushed ▢ \\\n",
    "29. ◯ owned ▢ \\\n",
    "\\\n",
    "For sentence 1) choosing from [A,B,C,D] best image is \"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 02: Test model with this `paper prompt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to evaluate against Richardson's findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /mounts/data/corp/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, OPTForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast, GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "\n",
    "with open(\"../../hf.key\", \"r\") as f_in:\n",
    "    hf_key = f_in.readline().strip()\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", hf_key])\n",
    "\n",
    "server_model_path = \"/mounts/data/corp/huggingface/\"\n",
    "model_type = \"meta-llama\"\n",
    "model_name = \"Llama-2-70b-hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fb01b9881c4328aceab5b86d1b2ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenizer = GPTNeoXTokenizerFast.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "#model = GPTNeoXForCausalLM.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "#model = OPTForCausalLM.from_pretrained(model_prefix+\"-\"+model_size, device_map=\"auto\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\", use_auth_token=True, device_map=\"auto\")\n",
    "#model = AutoModelForCausalLM.from_pretrained(server_model_path+\"llama/llama-13b\", use_auth_token=True, device_map=\"auto\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(server_model_path+model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Tokenize the prompt\n",
    "input_ids = tokenizer.encode(paper_prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Step 2: Generate the model input\n",
    "max_length = input_ids.size(1)  + 20 # Adjust '20' as needed to control the maximum length of the generated answer.\n",
    "output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "\n",
    "# Step 3: Decode the generated output to get the answer\n",
    "generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer:\n",
      "\n",
      " ◯→▢. For sentence 2) choosing from [A,B,\n"
     ]
    }
   ],
   "source": [
    "print(\"Generated answer:\\n\\n\", generated_answer[len(paper_prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 03: Prompting that is maximally friendly for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION: hunted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated answer:\n",
      "\n",
      " →▢].\n",
      "\n",
      "### \n",
      "ACTION: showed\n",
      "Generated answer:\n",
      "\n",
      " ←▢].\n",
      "\n",
      "### \n",
      "ACTION: succeeded\n",
      "Generated answer:\n",
      "\n",
      " →▢].\n",
      "\n",
      "### \n",
      "ACTION: pointed at\n",
      "Generated answer:\n",
      "\n",
      " ←▢].\n",
      "\n",
      "Select the image\n",
      "ACTION: pushed\n",
      "Generated answer:\n",
      "\n",
      " ←▢].\n",
      "\n",
      "Select the image\n",
      "ACTION: hoped\n",
      "Generated answer:\n",
      "\n",
      " \n",
      "↓\n",
      "▢].\n",
      "\n",
      "Select\n",
      "ACTION: walked\n",
      "Generated answer:\n",
      "\n",
      " →▢].\n",
      "\n",
      "### \n",
      "ACTION: gave to\n",
      "Generated answer:\n",
      "\n",
      " ←▢].\n",
      "\n",
      "Select the image\n",
      "ACTION: respected\n",
      "Generated answer:\n",
      "\n",
      " \n",
      "↓\n",
      "▢].\n",
      "\n",
      "Select\n",
      "ACTION: smashed\n",
      "Generated answer:\n",
      "\n",
      " →▢].\n",
      "\n",
      "Select the image\n"
     ]
    }
   ],
   "source": [
    "for action_word in action_words[:10]:\n",
    "    \n",
    "    print(\"ACTION: \"+action_word)\n",
    "\n",
    "    friendly_prompt = \"Select the image that best represents the event described by the sentence: \"+action_word+\"\\n[◯→▢]\\n\\n[◯←▢]\\n\\n[◯\\n↑\\n▢]\\n\\n[◯\\n↓\\n▢]\\n\\nThe best representation is [◯\"\n",
    "    \n",
    "    input_ids = tokenizer.encode(friendly_prompt, return_tensors=\"pt\")\n",
    "    max_length = input_ids.size(1)  + 10\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)  \n",
    "    print(\"Generated answer:\\n\\n\", generated_answer[len(friendly_prompt):])\n",
    "\n",
    "prompt_B = \"Select the image that best represents the event described by the sentence: \"+action_word+\"\\n[◯→▢]\\n\\n[◯←▢]\\n\\n[◯\\n↑\\n▢]\\n\\n[◯\\n↓\\n▢]\\n\\nThe best representation is [◯\"\n",
    "#prompt_C = \"Choose the best image for the word:\\nUP: ↑ \\nDOWN: ↓ \\nLEFT: → \\nRIGHT: ← \\n\"+action_word.upper()+\": \"\n",
    "#prompt_D = \"Choosing from UP, DOWN, LEFT and RIGHT, the word \\'\"+action_word.upper()+\"\\' is best respresented by the word \"\n",
    "#prompt_B = \"Of these four: A: ◯→▢ B: ◯←▢ C: ◯ ↑ ▢ D: ◯ ↓ ▢ Which one best describes \\\"◯ \"+action_word+\" ▢\\\" ?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 03: Test pipline with logprobs and `friendly prompt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sidemethods import logprobs_from_prompt, proc, proc_lower, prob_of_ending, calculate_accuracy, calculate_accuracies, store_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_prompt(action):\n",
    "    return \"Select the image that best represents the event described by the sentence: \"+action+\"\\n[◯→▢]\\n\\n[◯←▢]\\n\\n[◯\\n↑\\n▢]\\n\\n[◯\\n↓\\n▢]\\n\\nThe best representation is [◯\"\n",
    "    \n",
    "def get_test_action(country):\n",
    "    return country+\"'s captitol is \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fled': [7.2, 4.2, 80.8, 7.8],\n",
       " 'pointed at': [7.2, 3.6, 0.0, 89.2],\n",
       " 'pulled': [6.0, 5.4, 75.4, 13.2],\n",
       " 'pushed': [7.2, 3.6, 1.2, 8.0, 8.0],\n",
       " 'walked': [9.0, 3.6, 2.0, 4.0, 62.9],\n",
       " 'hunted': [9.6, 20.4, 1.8, 68.3],\n",
       " 'impacted': [7.2, 37.1, 3.0, 52.7],\n",
       " 'perched': [1.0, 2.0, 7.0, 6.0, 6.6, 5.4],\n",
       " 'showed': [1.0, 5.0, 9.0, 10.2, 65.9],\n",
       " 'smashed': [3.6, 66.5, 1.2, 28.7],\n",
       " 'bombed': [4.8, 86.8, 1.8, 6.6],\n",
       " 'flew': [37.7, 44.3, 15.0, 3.0],\n",
       " 'floated': [32.9, 56.3, 7.8, 3.0],\n",
       " 'lifted': [87.4, 9.6, 2.4, 0.6],\n",
       " 'sank': [22.2, 71.9, 4.2, 1.8],\n",
       " 'argued with': [11.4, 13.8, 12.6, 62.3],\n",
       " 'gave to': [8.4, 9.6, 1.2, 80.8],\n",
       " 'offended': [9.0, 31.7, 24.6, 34.7],\n",
       " 'rushed': [10.2, 10.8, 23.4, 55.1],\n",
       " 'warned': [10.8, 22.2, 6.0, 61.1],\n",
       " 'owned': [5.4, 55.7, 18.6, 20.4],\n",
       " 'regretted': [19.8, 24.0, 41.3, 1.0, 5.0],\n",
       " 'rested': [14.4, 36.5, 40.1, 9.0],\n",
       " 'tempted': [16.8, 11.4, 45.5, 26.3],\n",
       " 'wanted': [15.6, 7.8, 15.6, 61.1],\n",
       " 'hoped': [45.5, 15.6, 7.2, 31.7],\n",
       " 'increased': [73.7, 7.2, 9.6, 9.0],\n",
       " 'obeyed': [22.8, 4.2, 64.7, 8.4],\n",
       " 'respected': [53.9, 3.0, 14.4, 28.7],\n",
       " 'succeeded': [40.1, 35.9, 10.8, 13.2],\n",
       " 0: ['C:◯↑▢', 'D:◯↓▢', 'B:◯←▢ ', 'A:◯→▢']}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "richardson_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:46<00:00,  3.55s/it]\n"
     ]
    }
   ],
   "source": [
    "friendly_prompt_choices = dict()\n",
    "\n",
    "#action_words= [\"Germany\", \"Australia\", \"Japan\", \"Israel\", \"France\"]\n",
    "\n",
    "for action_word in tqdm(action_words):\n",
    "    #print(action_word)\n",
    "    if action_word == 0:\n",
    "        continue\n",
    "    \n",
    "    start = get_action_prompt(action_word)\n",
    "    #start = get_test_action(action_word)\n",
    "    \n",
    "    \n",
    "    answers = {0:\"→\", 1:\"←\", 2:\"↑\", 3:\"↓\"}\n",
    "    #answers = {0: \"Berlin\", 1:\"Paris\", 2:\"Canberra\", 3:\"Tokyo\"}\n",
    "\n",
    "    #start = \"nazis are known to be on the political \"\n",
    "    #answers = {0:\"UP\", 1:\"DOWN\", 2:\"LEFT\", 3:\"RIGHT\"}\n",
    "\n",
    "    res_ends = []\n",
    "    for j, end in answers.items():\n",
    "        input_prompt = proc(start) + ' ' + proc(end)\n",
    "        logprobs = logprobs_from_prompt(input_prompt, tokenizer, model)\n",
    "        res = {\"tokens\": [x for x,y in logprobs],\"token_logprobs\": [y for x,y in logprobs]}\n",
    "        res_ends.append(res)\n",
    "\n",
    "    choosen_answer = (-9999, \"\")\n",
    "    \n",
    "    choice_distribution = []\n",
    "    for i, answer in answers.items():\n",
    "        choice_val = prob_of_ending(res_ends[i]['token_logprobs'], res_ends[i]['tokens'])\n",
    "        if choice_val > choosen_answer[0]:\n",
    "            choosen_answer = choice_val, answer\n",
    "        choice_distribution.append(choice_val)\n",
    "    \n",
    "    # Richardson has table columns and image sequences mixed up, we align it here\n",
    "    friendly_prompt_choices[action_word] = []\n",
    "    friendly_prompt_choices[action_word].append(choice_distribution[2])\n",
    "    friendly_prompt_choices[action_word].append(choice_distribution[3])\n",
    "    friendly_prompt_choices[action_word].append(choice_distribution[1])\n",
    "    friendly_prompt_choices[action_word].append(choice_distribution[0])\n",
    "\n",
    "    #print(\"Choice: \", choosen_answer[1])\n",
    "    #print()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hunted [-0.78, -1.3, -1.65, -2.21]\n",
      "showed [-0.79, -1.39, -1.67, -2.22]\n",
      "succeeded [-0.82, -1.43, -1.72, -2.19]\n",
      "pointed at [-0.79, -1.42, -1.71, -2.19]\n",
      "pushed [-0.83, -1.42, -1.63, -2.07]\n",
      "hoped [-0.8, -1.38, -1.75, -2.29]\n",
      "walked [-0.78, -1.41, -1.64, -2.11]\n",
      "gave to [-0.83, -1.43, -1.66, -2.17]\n",
      "respected [-0.78, -1.41, -1.71, -2.28]\n",
      "smashed [-0.81, -1.4, -1.7, -2.18]\n",
      "argued with [-0.78, -1.31, -1.64, -2.33]\n",
      "sank [-0.81, -1.23, -1.76, -2.28]\n",
      "rested [-0.77, -1.37, -1.8, -2.25]\n",
      "offended [-0.79, -1.33, -1.75, -2.38]\n",
      "pulled [-0.8, -1.4, -1.65, -2.17]\n",
      "perched [-0.72, -1.38, -1.78, -2.32]\n",
      "regretted [-0.76, -1.35, -1.71, -2.33]\n",
      "bombed [-0.73, -1.35, -1.63, -2.18]\n",
      "obeyed [-0.79, -1.31, -1.69, -2.21]\n",
      "lifted [-0.76, -1.42, -1.79, -2.21]\n",
      "flew [-0.74, -1.54, -1.81, -2.14]\n",
      "impacted [-0.79, -1.32, -1.66, -2.03]\n",
      "wanted [-0.81, -1.41, -1.67, -2.15]\n",
      "increased [-0.83, -1.46, -1.93, -2.16]\n",
      "warned [-0.79, -1.43, -1.67, -2.24]\n",
      "floated [-0.76, -1.41, -1.8, -2.2]\n",
      "tempted [-0.78, -1.37, -1.75, -2.24]\n",
      "rushed [-0.79, -1.43, -1.79, -2.17]\n",
      "owned [-0.82, -1.38, -1.68, -2.22]\n"
     ]
    }
   ],
   "source": [
    "for k, v in friendly_prompt_choices.items():\n",
    "    print(k, [round(x,2) for x in v])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter how the friendly prompt is adjusted, the weight of the symbols  → is so strong that it will always heavily favor one symbol over all other symbols. It is not feasible to ask for the logprobs in this experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 04: Suitable `friendly prompt` for zero-shot evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/generation/utils.py:1468: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 31/31 [04:15<00:00,  8.23s/it]\n"
     ]
    }
   ],
   "source": [
    "model_choices = dict()\n",
    "arrows = ['↑', '↓', '←', '→']\n",
    "\n",
    "for action_word in tqdm(action_words):\n",
    "\n",
    "    if action_word == 0:\n",
    "        continue\n",
    "\n",
    "    friendly_prompt = \"Select the image that best represents the event described by the sentence: \"+action_word+\"\\n[◯→▢]\\n\\n[◯←▢]\\n\\n[◯\\n↑\\n▢]\\n\\n[◯\\n↓\\n▢]\\n\\nThe best representation is [◯\"\n",
    "    \n",
    "    input_ids = tokenizer.encode(friendly_prompt, return_tensors=\"pt\")\n",
    "    max_length = input_ids.size(1)  + 20\n",
    "    output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)  \n",
    "    \n",
    "    model_choices[action_word] = []\n",
    "    for idx, arrow in enumerate(arrows):\n",
    "        if arrow in generated_answer[len(friendly_prompt):]:\n",
    "            model_choices[action_word].append(1)\n",
    "        else:\n",
    "            model_choices[action_word].append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action C:◯↑▢ D:◯↓▢ B:◯←▢  A:◯→▢\n",
      "walked [0, 0, 0, 1]\n",
      "perched [0, 1, 0, 0]\n",
      "respected [0, 1, 0, 0]\n",
      "fled [0, 0, 1, 0]\n",
      "pulled [0, 0, 1, 0]\n",
      "wanted [0, 0, 0, 1]\n",
      "pushed [0, 0, 1, 0]\n",
      "argued with [0, 0, 1, 0]\n",
      "obeyed [0, 0, 1, 0]\n",
      "showed [0, 0, 1, 0]\n",
      "sank [0, 1, 0, 0]\n",
      "lifted [0, 0, 1, 0]\n",
      "regretted [0, 0, 1, 0]\n",
      "gave to [0, 0, 1, 0]\n",
      "pointed at [0, 0, 1, 0]\n",
      "impacted [0, 0, 1, 0]\n",
      "owned [0, 0, 1, 0]\n",
      "smashed [0, 0, 0, 1]\n",
      "increased [0, 0, 1, 0]\n",
      "floated [0, 0, 0, 1]\n",
      "bombed [0, 0, 0, 1]\n",
      "hunted [0, 0, 0, 1]\n",
      "tempted [0, 0, 0, 1]\n",
      "hoped [0, 1, 0, 0]\n",
      "rushed [0, 0, 0, 1]\n",
      "flew [0, 0, 0, 1]\n",
      "rested [0, 0, 1, 0]\n",
      "offended [0, 0, 1, 0]\n",
      "succeeded [0, 0, 0, 1]\n",
      "warned [0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Action\", \"C:◯↑▢\", 'D:◯↓▢', 'B:◯←▢ ', 'A:◯→▢')\n",
    "for k,v in model_choices.items():\n",
    "    print(k,v)\n",
    "\n",
    "\n",
    "with open(\"results/exp01a_\"+model_name+\".txt\", \"w\") as f_out:\n",
    "    f_out.write(\"Action\\tC:◯↑▢\\tD:◯↓▢\\tB:◯←▢\\tA:◯→▢\\n\")\n",
    "    for k,v in model_choices.items():\n",
    "        f_out.write(k+\"\\t\"+\"\\t\".join([str(x) for x in v])+\"\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 05: Compare with Richardson's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# binarize richardson's data\n",
    "with open(\"results/exp01_richardson_categorial.txt\", \"w\") as f_out:\n",
    "    f_out.write(\"Action\\tC:◯↑▢\\tD:◯↓▢\\tB:◯←▢\\tA:◯→▢\\n\")\n",
    "    for k, v in richardson_data.items():\n",
    "        vals = [0,0,0,0]\n",
    "        vals[v.index(max(v))] = 1\n",
    "        f_out.write(k+\"\\t\"+\"\\t\".join([str(x) for x in vals])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
