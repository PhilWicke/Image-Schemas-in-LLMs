{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 01\n",
    "## with textual encoding (UP, DOWN, LEFT, RIGHT)\n",
    "### Richardson et. al (2002) Experiment 01 with Language Models instead of Humans:\n",
    "\n",
    "The subjects were presented with a single page,\n",
    "containing a list of the verbs and four pictures, labelled A to\n",
    "D. Each one contained a circle and a square aligned along a\n",
    "vertical or horizontal axis, connected by an arrow pointing\n",
    "up, down, left or right. Since we didn't expect any\n",
    "interesting item variation between left or right placement of\n",
    "the circle or square, the horizontal schemas differed only in\n",
    "the direction of the arrow.\n",
    "For each sentence, subjects were asked to select one of\n",
    "the four sparse images that best depicted the event described\n",
    "by the sentence (Figure 1)\n",
    "The items were randomised in three different orders, and\n",
    "crossed with two different orderings of the images. The six\n",
    "lists were then distributed randomly to subjects.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of Experimental Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /mounts/data/corp/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import GPT2Tokenizer, OPTForCausalLM, AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import GPTNeoXForCausalLM, GPTNeoXTokenizerFast, GPTNeoForCausalLM, GPT2Tokenizer\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "import subprocess\n",
    "import random\n",
    "random.seed(1337)\n",
    "\n",
    "with open(\"../../hf.key\", \"r\") as f_in:\n",
    "    hf_key = f_in.readline().strip()\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", hf_key])\n",
    "\n",
    "def convert_to_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except ValueError:\n",
    "        return value\n",
    "\n",
    "server_model_path = \"/mounts/data/corp/huggingface/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading preprocessed data by Richardson\n",
    "\n",
    "Creates three dictionaries:\n",
    " * `richardson_data`\n",
    " \n",
    " All choices as vectors, e.g. `{'fled': [7.2, 4.2, 80.8, 7.8], 'pointed at': [7.2, 3.6, 0.0, 89.2] ...`\n",
    " \n",
    " * `richardson_categorial`\n",
    " \n",
    " Maximum choice as binary choice, e.g. `{'fled': [0, 0, 1, 0], 'pointed at': [0, 0, 0, 1] ...`\n",
    " \n",
    " * `richardson_normed`\n",
    " \n",
    " Maximum choice divided by all choices, disregarding all other choices, e.g.  `{'fled': [0.0, 0.0, 0.808, 0.0], 'pointed at': [0.0, 0.0, 0.0, 0.892] ...`\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fled': [0.072, 0.042, 0.808, 0.078], 'pointed at': [0.072, 0.036, 0.0, 0.892], 'pulled': [0.06, 0.054, 0.754, 0.132], 'pushed': [0.072, 0.036, 0.012, 0.88], 'walked': [0.0905, 0.0362, 0.2412, 0.6322], 'hunted': [0.0959, 0.2038, 0.018, 0.6823], 'impacted': [0.072, 0.371, 0.03, 0.527], 'perched': [0.12, 0.76, 0.066, 0.054], 'showed': [0.1499, 0.0899, 0.1019, 0.6583], 'smashed': [0.036, 0.665, 0.012, 0.287], 'bombed': [0.048, 0.868, 0.018, 0.066], 'flew': [0.377, 0.443, 0.15, 0.03], 'floated': [0.329, 0.563, 0.078, 0.03], 'lifted': [0.874, 0.096, 0.024, 0.006], 'sank': [0.2218, 0.7183, 0.042, 0.018], 'argued with': [0.1139, 0.1379, 0.1259, 0.6224], 'gave to': [0.084, 0.096, 0.012, 0.808], 'offended': [0.09, 0.317, 0.246, 0.347], 'rushed': [0.1025, 0.1085, 0.2352, 0.5538], 'warned': [0.1079, 0.2218, 0.0599, 0.6104], 'owned': [0.0539, 0.5564, 0.1858, 0.2038], 'regretted': [0.1978, 0.2398, 0.4126, 0.1499], 'rested': [0.144, 0.365, 0.401, 0.09], 'tempted': [0.168, 0.114, 0.455, 0.263], 'wanted': [0.1558, 0.0779, 0.1558, 0.6104], 'hoped': [0.455, 0.156, 0.072, 0.317], 'increased': [0.7407, 0.0724, 0.0965, 0.0905], 'obeyed': [0.2278, 0.042, 0.6464, 0.0839], 'respected': [0.539, 0.03, 0.144, 0.287], 'succeeded': [0.401, 0.359, 0.108, 0.132]}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../../data/richardson_actions.txt\", \"r\") as d_in:\n",
    "    lines = [line.split() for line in d_in.readlines()]\n",
    "\n",
    "output = []\n",
    "for entry in lines:\n",
    "    new_entry = [convert_to_float(item) for item in entry]\n",
    "    \n",
    "    if isinstance(new_entry[1],str):\n",
    "        new_entry[0] = \" \".join(new_entry[:2])\n",
    "        del new_entry[1]\n",
    "    output.append(new_entry)\n",
    "\n",
    "richardson_data = dict()\n",
    "for elem in output:\n",
    "    richardson_data[elem[0]] = [i for i in elem[1:]]\n",
    "\n",
    "# Randomizing Richardson's data\n",
    "action_words = list(richardson_data.keys())\n",
    "random.shuffle(action_words)\n",
    "\n",
    "richardson_categorial = dict()\n",
    "for k, v in richardson_data.items():\n",
    "    if k == 0:\n",
    "        continue\n",
    "    vals = [0,0,0,0]\n",
    "    vals[v.index(max(v))] = 1\n",
    "\n",
    "    richardson_categorial[k] = vals\n",
    "richardson_normed = dict()\n",
    "\n",
    "for action, values in richardson_data.items():\n",
    "    if action == 0:\n",
    "        continue\n",
    "    \n",
    "    richardson_normed[action] = [round(val/sum(values),4) for val in values]\n",
    "\n",
    "print(richardson_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"meta-llama\"\n",
    "model_name = \"Llama-2-7b-hf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose specifc GPU for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GPU ID you want to use\n",
    "gpu_id = 6\n",
    "\n",
    "# Use the torch.cuda.device() context manager to set the current GPU\n",
    "with torch.cuda.device(gpu_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_type+\"/\"+model_name, use_auth_token=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(server_model_path+model_type+\"/\"+model_name, use_auth_token=True).to(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "/mounts/work/pwicke/miniconda3/envs/schemas/lib/python3.8/site-packages/transformers/modeling_utils.py:2193: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ee5e87f83a44378bee09307d166dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpu_id = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(server_model_path+model_type+\"/\"+model_name, use_auth_token=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run experiment (inludes prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:51<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "model_choices = dict()\n",
    "arrows = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "for action_word in tqdm(action_words):\n",
    "\n",
    "    if action_word == 0:\n",
    "        continue\n",
    "\n",
    "    ### PROMPT DEFINED HERE\n",
    "    friendly_prompt = \"Select the CONCEPT that best represents the event described by the sentence: \"+action_word+\". CONCEPTS: UP, DOWN, LEFT, RIGHT.\\nThe best representation is CONCEPT:\"\n",
    "    \n",
    "    if gpu_id:\n",
    "        input_ids = tokenizer.encode(friendly_prompt, return_tensors=\"pt\").to(torch.device(\"cuda\"))\n",
    "        max_length = input_ids.size(1)  + 20\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1).to(torch.device(\"cuda\"))\n",
    "    else:\n",
    "        input_ids = tokenizer.encode(friendly_prompt, return_tensors=\"pt\")\n",
    "        max_length = input_ids.size(1)  + 20\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
    "    generated_answer = tokenizer.decode(output[0], skip_special_tokens=True)  \n",
    "    \n",
    "    model_choices[action_word] = []\n",
    "    for idx, arrow in enumerate(arrows):\n",
    "        if arrow in generated_answer[len(friendly_prompt):]:\n",
    "            model_choices[action_word].append(1)\n",
    "        else:\n",
    "            model_choices[action_word].append(0)\n",
    "\n",
    "# Llama-70b: 4m:15s\n",
    "# Llama-13b: 1m:50s\n",
    "# Llama-7b:  40s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing experimental results from free-form generation as `exp01a_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_store = input(\"Should the result for \"+model_name+\" be stored? (y/n):\")\n",
    "\n",
    "if to_store == \"y\":\n",
    "\n",
    "    with open(\"results/exp01b_\"+model_name+\".txt\", \"w\") as f_out:\n",
    "        f_out.write(\"Action\\tUP\\tDOWN\\tLEFT\\tRIGHT\\n\")\n",
    "        for k,v in model_choices.items():\n",
    "            f_out.write(k+\"\\t\"+\"\\t\".join([str(x) for x in v])+\"\\n\")\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'walked': [1, 0, 0, 0],\n",
       " 'perched': [1, 0, 0, 0],\n",
       " 'respected': [1, 0, 0, 0],\n",
       " 'fled': [1, 0, 0, 0],\n",
       " 'pulled': [0, 1, 0, 0],\n",
       " 'wanted': [1, 0, 0, 0],\n",
       " 'pushed': [0, 1, 0, 0],\n",
       " 'argued with': [1, 0, 0, 0],\n",
       " 'obeyed': [1, 0, 0, 0],\n",
       " 'showed': [1, 0, 0, 0],\n",
       " 'sank': [0, 1, 0, 0],\n",
       " 'lifted': [1, 0, 0, 0],\n",
       " 'regretted': [1, 0, 0, 0],\n",
       " 'gave to': [1, 0, 0, 0],\n",
       " 'pointed at': [1, 0, 0, 0],\n",
       " 'succeeded': [1, 0, 0, 0],\n",
       " 'impacted': [1, 0, 0, 0],\n",
       " 'owned': [1, 0, 0, 0],\n",
       " 'smashed': [1, 0, 0, 0],\n",
       " 'increased': [1, 0, 0, 0],\n",
       " 'floated': [1, 0, 0, 0],\n",
       " 'bombed': [0, 1, 0, 0],\n",
       " 'hunted': [1, 0, 0, 0],\n",
       " 'tempted': [1, 0, 0, 0],\n",
       " 'hoped': [1, 0, 0, 0],\n",
       " 'rushed': [1, 0, 0, 0],\n",
       " 'flew': [1, 0, 0, 0],\n",
       " 'rested': [1, 0, 0, 0],\n",
       " 'offended': [1, 0, 0, 0],\n",
       " 'warned': [1, 0, 0, 0]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_choices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
