{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/Users/cisintern/pwicke/.local/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "from sidemethods import load_richardson_data, convert_to_float\n",
    "from PIL import Image\n",
    "from transformers import IdeficsForVisionText2Text, AutoProcessor, BitsAndBytesConfig\n",
    "\n",
    "# local path to model repository on our servers\n",
    "server_model_path = \"/mounts/data/corp/huggingface/\"\n",
    "# loading the original human data as vectors for each action word\n",
    "_, _, richardson_normed = load_richardson_data()\n",
    "action_words = richardson_normed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_inference(model, processor, prompts, max_new_tokens=50):\n",
    "    tokenizer = processor.tokenizer\n",
    "    bad_words = [\"<image>\", \"<fake_token_around_image>\"]\n",
    "    if len(bad_words) > 0:\n",
    "        bad_words_ids = tokenizer(bad_words, add_special_tokens=False).input_ids\n",
    "\n",
    "    eos_token = \"</s>\"\n",
    "    eos_token_id = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "\n",
    "    inputs = processor(prompts, return_tensors=\"pt\").to(device)\n",
    "    generated_ids = model.generate(**inputs, eos_token_id=[eos_token_id], bad_words_ids=bad_words_ids, max_new_tokens=max_new_tokens, early_stopping=True)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /mounts/Users/cisintern/pwicke/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "with open(\"../../hf.key\", \"r\") as f_in:\n",
    "    hf_key = f_in.readline().strip()\n",
    "\n",
    "login(token = hf_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'huggingface-cli'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image Schemas in LLMs/src/experiment_01/A_Unicode-Experiment.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m../../hf.key\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f_in:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     hf_key \u001b[39m=\u001b[39m f_in\u001b[39m.\u001b[39mreadline()\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m subprocess\u001b[39m.\u001b[39;49mrun([\u001b[39m\"\u001b[39;49m\u001b[39mhuggingface-cli\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mlogin\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m--token\u001b[39;49m\u001b[39m\"\u001b[39;49m, hf_key])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mHuggingFaceM4/tiny-random-idefics\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39mfrom_pretrained(checkpoint, use_auth_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/subprocess.py:503\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstdout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[1;32m    501\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m PIPE\n\u001b[0;32m--> 503\u001b[0m \u001b[39mwith\u001b[39;00m Popen(\u001b[39m*\u001b[39;49mpopenargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mas\u001b[39;00m process:\n\u001b[1;32m    504\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    505\u001b[0m         stdout, stderr \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mcommunicate(\u001b[39minput\u001b[39m, timeout\u001b[39m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/subprocess.py:971\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_mode:\n\u001b[1;32m    968\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    969\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 971\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    972\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    973\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    974\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    975\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    976\u001b[0m                         errread, errwrite,\n\u001b[1;32m    977\u001b[0m                         restore_signals,\n\u001b[1;32m    978\u001b[0m                         gid, gids, uid, umask,\n\u001b[1;32m    979\u001b[0m                         start_new_session)\n\u001b[1;32m    980\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    982\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mfilter\u001b[39m(\u001b[39mNone\u001b[39;00m, (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdin, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstdout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/subprocess.py:1863\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     \u001b[39mif\u001b[39;00m errno_num \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1862\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1864\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'huggingface-cli'"
     ]
    }
   ],
   "source": [
    "device = 2\n",
    "\n",
    "checkpoint = \"HuggingFaceM4/tiny-random-idefics\"\n",
    "processor = AutoProcessor.from_pretrained(checkpoint, use_auth_token=True)\n",
    "model = IdeficsForVisionText2Text.from_pretrained(checkpoint ,device_map={\"\":device}) #  quantization_config=bnb_config,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/models/auto/processing_auto.py:206: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards:   6%|â–Œ         | 1/17 [00:30<08:06, 30.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image Schemas in LLMs/src/experiment_01/A_Unicode-Experiment.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m processor \u001b[39m=\u001b[39m AutoProcessor\u001b[39m.\u001b[39mfrom_pretrained(checkpoint, use_auth_token\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Simply take-off the quantization_config arg if you want to load the original model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m model \u001b[39m=\u001b[39m IdeficsForVisionText2Text\u001b[39m.\u001b[39;49mfrom_pretrained(checkpoint,quantization_config\u001b[39m=\u001b[39;49mbnb_config ,device_map\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m:device}) \u001b[39m#  quantization_config=bnb_config,\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/modeling_utils.py:3333\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3323\u001b[0m     \u001b[39mif\u001b[39;00m dtype_orig \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3324\u001b[0m         torch\u001b[39m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   3326\u001b[0m     (\n\u001b[1;32m   3327\u001b[0m         model,\n\u001b[1;32m   3328\u001b[0m         missing_keys,\n\u001b[1;32m   3329\u001b[0m         unexpected_keys,\n\u001b[1;32m   3330\u001b[0m         mismatched_keys,\n\u001b[1;32m   3331\u001b[0m         offload_index,\n\u001b[1;32m   3332\u001b[0m         error_msgs,\n\u001b[0;32m-> 3333\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_load_pretrained_model(\n\u001b[1;32m   3334\u001b[0m         model,\n\u001b[1;32m   3335\u001b[0m         state_dict,\n\u001b[1;32m   3336\u001b[0m         loaded_state_dict_keys,  \u001b[39m# XXX: rename?\u001b[39;49;00m\n\u001b[1;32m   3337\u001b[0m         resolved_archive_file,\n\u001b[1;32m   3338\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3339\u001b[0m         ignore_mismatched_sizes\u001b[39m=\u001b[39;49mignore_mismatched_sizes,\n\u001b[1;32m   3340\u001b[0m         sharded_metadata\u001b[39m=\u001b[39;49msharded_metadata,\n\u001b[1;32m   3341\u001b[0m         _fast_init\u001b[39m=\u001b[39;49m_fast_init,\n\u001b[1;32m   3342\u001b[0m         low_cpu_mem_usage\u001b[39m=\u001b[39;49mlow_cpu_mem_usage,\n\u001b[1;32m   3343\u001b[0m         device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3344\u001b[0m         offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3345\u001b[0m         offload_state_dict\u001b[39m=\u001b[39;49moffload_state_dict,\n\u001b[1;32m   3346\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch_dtype,\n\u001b[1;32m   3347\u001b[0m         is_quantized\u001b[39m=\u001b[39;49m(\u001b[39mgetattr\u001b[39;49m(model, \u001b[39m\"\u001b[39;49m\u001b[39mquantization_method\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m) \u001b[39m==\u001b[39;49m QuantizationMethod\u001b[39m.\u001b[39;49mBITS_AND_BYTES),\n\u001b[1;32m   3348\u001b[0m         keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3349\u001b[0m     )\n\u001b[1;32m   3351\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_4bit \u001b[39m=\u001b[39m load_in_4bit\n\u001b[1;32m   3352\u001b[0m model\u001b[39m.\u001b[39mis_loaded_in_8bit \u001b[39m=\u001b[39m load_in_8bit\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/modeling_utils.py:3723\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, loaded_keys, resolved_archive_file, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, _fast_init, low_cpu_mem_usage, device_map, offload_folder, offload_state_dict, dtype, is_quantized, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m   3721\u001b[0m \u001b[39mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m   3722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fsdp_enabled() \u001b[39mor\u001b[39;00m is_fsdp_enabled_and_dist_rank_0():\n\u001b[0;32m-> 3723\u001b[0m         new_error_msgs, offload_index, state_dict_index \u001b[39m=\u001b[39m _load_state_dict_into_meta_model(\n\u001b[1;32m   3724\u001b[0m             model_to_load,\n\u001b[1;32m   3725\u001b[0m             state_dict,\n\u001b[1;32m   3726\u001b[0m             loaded_keys,\n\u001b[1;32m   3727\u001b[0m             start_prefix,\n\u001b[1;32m   3728\u001b[0m             expected_keys,\n\u001b[1;32m   3729\u001b[0m             device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m   3730\u001b[0m             offload_folder\u001b[39m=\u001b[39;49moffload_folder,\n\u001b[1;32m   3731\u001b[0m             offload_index\u001b[39m=\u001b[39;49moffload_index,\n\u001b[1;32m   3732\u001b[0m             state_dict_folder\u001b[39m=\u001b[39;49mstate_dict_folder,\n\u001b[1;32m   3733\u001b[0m             state_dict_index\u001b[39m=\u001b[39;49mstate_dict_index,\n\u001b[1;32m   3734\u001b[0m             dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   3735\u001b[0m             is_quantized\u001b[39m=\u001b[39;49mis_quantized,\n\u001b[1;32m   3736\u001b[0m             is_safetensors\u001b[39m=\u001b[39;49mis_safetensors,\n\u001b[1;32m   3737\u001b[0m             keep_in_fp32_modules\u001b[39m=\u001b[39;49mkeep_in_fp32_modules,\n\u001b[1;32m   3738\u001b[0m         )\n\u001b[1;32m   3739\u001b[0m         error_msgs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m new_error_msgs\n\u001b[1;32m   3740\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/modeling_utils.py:709\u001b[0m, in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, loaded_state_dict_keys, start_prefix, expected_keys, device_map, offload_folder, offload_index, state_dict_folder, state_dict_index, dtype, is_quantized, is_safetensors, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    707\u001b[0m             set_module_kwargs[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32\n\u001b[1;32m    708\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 709\u001b[0m         param \u001b[39m=\u001b[39m param\u001b[39m.\u001b[39;49mto(dtype)\n\u001b[1;32m    711\u001b[0m \u001b[39m# For compatibility with PyTorch load_state_dict which converts state dict dtype to existing dtype in model\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[39mif\u001b[39;00m dtype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 2\n",
    "\n",
    "# checkpoint = \"HuggingFaceM4/tiny-random-idefics\"\n",
    "local_path = \"/mounts/data/corp/huggingface/\"\n",
    "checkpoint = local_path+\"idefics/idefics-80b\"\n",
    "\n",
    "# Here we skip some special modules that can't be quantized properly\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    llm_int8_skip_modules=[\"lm_head\", \"embed_tokens\"],\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(checkpoint, use_auth_token=True)\n",
    "# Simply take-off the quantization_config arg if you want to load the original model\n",
    "model = IdeficsForVisionText2Text.from_pretrained(checkpoint,quantization_config=bnb_config ,device_map={\"\":device}) #  quantization_config=bnb_config,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:418: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: In the image, which direction does the arrow point towards to? Answer: The arrow points towards the right.\n",
      "Question: In the image, which direction does the arrow point towards to? Answer: The arrow points towards the right.\n",
      "Question: In the image, which direction does the arrow point towards to? Answer: The arrow points towards the right.\n",
      "Question: In the image, which direction does the arrow point towards to? Answer: Upwards\n",
      "\n",
      "Question: Which direction is depicted? Answer: The arrow is pointing to the right.\n",
      "Question: Which direction is depicted? Answer: The arrow is pointing to the right.\n",
      "Question: Which direction is depicted? Answer: The direction of the force is from the left to the right.\n",
      "Question: Which direction is depicted? Answer: The direction of the force is from the box to the person.\n",
      "\n",
      "Which direction does the arrow point towards to? Answer: The arrow points towards the right.\n",
      "Which direction does the arrow point towards to? Answer: The arrow points towards the object.\n",
      "Which direction does the arrow point towards to? Answer: The arrow points towards the right.\n",
      "Which direction does the arrow point towards to? Answer: Upwards\n",
      "\n",
      "The image shows a diagram of a line segment with a circle at one end and a square at the other.\n",
      "The image shows a diagram of a simple system. The diagram shows a circle with an arrow pointing to the right. The arrow is connected to\n",
      "The image shows the pendulum of a pendulum clock.\n",
      "The image shows a simple pendulum. The pendulum is a mass attached to a string. The string is attached to\n"
     ]
    }
   ],
   "source": [
    "img_path = \"test_images/put-block-in-matching-bowl_3.png\"\n",
    "\n",
    "for i in range(1,5):\n",
    "    prompts = [Image.open(\"../../data/direction0\"+str(i)+\".png\").convert(\"RGB\"),\n",
    "        \"Question: In the image, which direction does the arrow point towards to? Answer:\",]\n",
    "    check_inference(model, processor, prompts, max_new_tokens=25)\n",
    "print()\n",
    "for i in range(1,5):\n",
    "    prompts = [Image.open(\"../../data/direction0\"+str(i)+\".png\").convert(\"RGB\"),\n",
    "        \"Question: Which direction is depicted? Answer:\",]\n",
    "    check_inference(model, processor, prompts, max_new_tokens=25)\n",
    "print()\n",
    "for i in range(1,5):\n",
    "    prompts = [Image.open(\"../../data/direction0\"+str(i)+\".png\").convert(\"RGB\"),\n",
    "        \"Which direction does the arrow point towards to? Answer:\",]\n",
    "    check_inference(model, processor, prompts, max_new_tokens=25)\n",
    "print()\n",
    "for i in range(1,5):\n",
    "    prompts = [Image.open(\"../../data/direction0\"+str(i)+\".png\").convert(\"RGB\"),\n",
    "        \"The image shows\",]\n",
    "    check_inference(model, processor, prompts, max_new_tokens=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fled\n",
      "direction02 is KYMD\n",
      "direction01 is PWTX\n",
      "direction04 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle fled square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle fled square' is best\n",
      "\n",
      "pointed at\n",
      "direction01 is KYMD\n",
      "direction04 is PWTX\n",
      "direction03 is YZPR\n",
      "direction02 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle pointed at square' best? Answer: Image 1.\n",
      "\n",
      "pulled\n",
      "direction02 is KYMD\n",
      "direction04 is PWTX\n",
      "direction03 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle pulled square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle pulled square' is best\n",
      "\n",
      "pushed\n",
      "direction03 is KYMD\n",
      "direction01 is PWTX\n",
      "direction02 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle pushed square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle pushed square' is best\n",
      "\n",
      "walked\n",
      "direction01 is KYMD\n",
      "direction02 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle walked square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle walked square' is best\n",
      "\n",
      "hunted\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction01 is YZPR\n",
      "direction02 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle hunted square' best? Answer: Image 1.\n",
      "\n",
      "Image KYMD.Question: Which of\n",
      "\n",
      "impacted\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction01 is YZPR\n",
      "direction02 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle impacted square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle impacted square' is\n",
      "\n",
      "perched\n",
      "direction04 is KYMD\n",
      "direction02 is PWTX\n",
      "direction03 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle perched square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle perched square' is\n",
      "\n",
      "showed\n",
      "direction02 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle showed square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle showed square' is best\n",
      "\n",
      "smashed\n",
      "direction03 is KYMD\n",
      "direction02 is PWTX\n",
      "direction01 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle smashed square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle smashed square' is\n",
      "\n",
      "bombed\n",
      "direction02 is KYMD\n",
      "direction03 is PWTX\n",
      "direction01 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle bombed square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle bombed square' is\n",
      "\n",
      "flew\n",
      "direction01 is KYMD\n",
      "direction02 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle flew square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle flew square' is\n",
      "\n",
      "floated\n",
      "direction02 is KYMD\n",
      "direction04 is PWTX\n",
      "direction01 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle floated square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle floated square' is\n",
      "\n",
      "lifted\n",
      "direction03 is KYMD\n",
      "direction01 is PWTX\n",
      "direction02 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.Question: Which of the images presents the event 'circle lifted square' best? Answer: Image 1.\n",
      "\n",
      "The event 'circle lifted square' is best\n",
      "\n",
      "sank\n",
      "direction02 is KYMD\n",
      "direction03 is PWTX\n",
      "direction04 is YZPR\n",
      "direction01 is DHNV\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image Schemas in LLMs/src/experiment_01/A_Unicode-Experiment.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     prompt\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mImage \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39mletters[idx]\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m prompt\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39m Question: Which of the images presents the event \u001b[39m\u001b[39m'\u001b[39m\u001b[39mcircle \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39maction_word\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m square\u001b[39m\u001b[39m'\u001b[39m\u001b[39m best? Answer:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m check_inference(model, processor, prompt, max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mprint\u001b[39m()\n",
      "\u001b[1;32m/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image Schemas in LLMs/src/experiment_01/A_Unicode-Experiment.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m eos_token_id \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mconvert_tokens_to_ids(eos_token)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m inputs \u001b[39m=\u001b[39m processor(prompts, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m generated_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, eos_token_id\u001b[39m=\u001b[39;49m[eos_token_id], bad_words_ids\u001b[39m=\u001b[39;49mbad_words_ids, max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens, early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m generated_text \u001b[39m=\u001b[39m processor\u001b[39m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bbeta.cis.uni-muenchen.de/mounts/Users/cisintern/pwicke/projects/05_ImageSchemas/Image%20Schemas%20in%20LLMs/src/experiment_01/A_Unicode-Experiment.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(generated_text)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/generation/utils.py:1658\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39massisted_decoding(\n\u001b[1;32m   1642\u001b[0m         input_ids,\n\u001b[1;32m   1643\u001b[0m         assistant_model\u001b[39m=\u001b[39massistant_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1654\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1655\u001b[0m     )\n\u001b[1;32m   1656\u001b[0m \u001b[39mif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mGREEDY_SEARCH:\n\u001b[1;32m   1657\u001b[0m     \u001b[39m# 11. run greedy search\u001b[39;00m\n\u001b[0;32m-> 1658\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgreedy_search(\n\u001b[1;32m   1659\u001b[0m         input_ids,\n\u001b[1;32m   1660\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1661\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1662\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1663\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1664\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1665\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1666\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1667\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1668\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1669\u001b[0m     )\n\u001b[1;32m   1671\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mCONTRASTIVE_SEARCH:\n\u001b[1;32m   1672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39muse_cache\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/generation/utils.py:2506\u001b[0m, in \u001b[0;36mGenerationMixin.greedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2503\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2505\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2506\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2507\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2508\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2509\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2510\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2511\u001b[0m )\n\u001b[1;32m   2513\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2514\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/models/idefics/modeling_idefics.py:1517\u001b[0m, in \u001b[0;36mIdeficsForVisionText2Text.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, image_encoder_embeddings, perceiver_embeddings, image_attention_mask, labels, use_cache, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m   1514\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m   1516\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1517\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   1518\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m   1519\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1520\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1521\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1522\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1523\u001b[0m     pixel_values\u001b[39m=\u001b[39;49mpixel_values,\n\u001b[1;32m   1524\u001b[0m     image_encoder_embeddings\u001b[39m=\u001b[39;49mimage_encoder_embeddings,\n\u001b[1;32m   1525\u001b[0m     perceiver_embeddings\u001b[39m=\u001b[39;49mperceiver_embeddings,\n\u001b[1;32m   1526\u001b[0m     image_attention_mask\u001b[39m=\u001b[39;49mimage_attention_mask,\n\u001b[1;32m   1527\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1528\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1529\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1530\u001b[0m     interpolate_pos_encoding\u001b[39m=\u001b[39;49minterpolate_pos_encoding,\n\u001b[1;32m   1531\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1532\u001b[0m )\n\u001b[1;32m   1534\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1535\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/models/idefics/modeling_idefics.py:1359\u001b[0m, in \u001b[0;36mIdeficsModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, image_encoder_embeddings, perceiver_embeddings, image_attention_mask, use_cache, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   1343\u001b[0m         vblock,\n\u001b[1;32m   1344\u001b[0m         decoder_layer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgated_cross_attn_layers,\n\u001b[1;32m   1357\u001b[0m     )\n\u001b[1;32m   1358\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1359\u001b[0m     layer_outputs \u001b[39m=\u001b[39m vblock(\n\u001b[1;32m   1360\u001b[0m         decoder_layer,\n\u001b[1;32m   1361\u001b[0m         hidden_states,\n\u001b[1;32m   1362\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1363\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1364\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1365\u001b[0m         image_hidden_states\u001b[39m=\u001b[39;49mimage_hidden_states,\n\u001b[1;32m   1366\u001b[0m         image_attention_mask\u001b[39m=\u001b[39;49mimage_attention_mask,\n\u001b[1;32m   1367\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1368\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1369\u001b[0m         no_images\u001b[39m=\u001b[39;49mno_images,\n\u001b[1;32m   1370\u001b[0m         layer_idx\u001b[39m=\u001b[39;49midx,\n\u001b[1;32m   1371\u001b[0m         cross_layer_interval\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_layer_interval,\n\u001b[1;32m   1372\u001b[0m         gated_cross_attn_layers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgated_cross_attn_layers,\n\u001b[1;32m   1373\u001b[0m     )\n\u001b[1;32m   1375\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1377\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/models/idefics/modeling_idefics.py:1311\u001b[0m, in \u001b[0;36mIdeficsModel.forward.<locals>.vblock\u001b[0;34m(main_block, hidden_states, attention_mask, position_ids, past_key_value, image_hidden_states, image_attention_mask, output_attentions, use_cache, no_images, layer_idx, cross_layer_interval, gated_cross_attn_layers)\u001b[0m\n\u001b[1;32m   1309\u001b[0m \u001b[39mif\u001b[39;00m layer_idx \u001b[39m%\u001b[39m cross_layer_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1310\u001b[0m     xblock \u001b[39m=\u001b[39m gated_cross_attn_layers[layer_idx \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m cross_layer_interval]\n\u001b[0;32m-> 1311\u001b[0m     outputs \u001b[39m=\u001b[39m xblock(\n\u001b[1;32m   1312\u001b[0m         hidden_states,\n\u001b[1;32m   1313\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1314\u001b[0m         image_hidden_states\u001b[39m=\u001b[39;49mimage_hidden_states,\n\u001b[1;32m   1315\u001b[0m         image_attention_mask\u001b[39m=\u001b[39;49mimage_attention_mask,\n\u001b[1;32m   1316\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1317\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1318\u001b[0m         past_key_value\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# not implemented\u001b[39;49;00m\n\u001b[1;32m   1319\u001b[0m         no_images\u001b[39m=\u001b[39;49mno_images,\n\u001b[1;32m   1320\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m     hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1323\u001b[0m layer_outputs \u001b[39m=\u001b[39m main_block(\n\u001b[1;32m   1324\u001b[0m     hidden_states,\n\u001b[1;32m   1325\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1329\u001b[0m     use_cache\u001b[39m=\u001b[39muse_cache,\n\u001b[1;32m   1330\u001b[0m )\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/models/idefics/modeling_idefics.py:911\u001b[0m, in \u001b[0;36mIdeficsGatedCrossAttentionLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, image_hidden_states, image_attention_mask, output_attentions, use_cache, past_key_value, no_images)\u001b[0m\n\u001b[1;32m    908\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    910\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 911\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcross_attn(\n\u001b[1;32m    912\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    913\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mimage_hidden_states,\n\u001b[1;32m    914\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mimage_attention_mask,\n\u001b[1;32m    915\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    916\u001b[0m )\n\u001b[1;32m    917\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    918\u001b[0m \u001b[39m# when there are no images the model is used in pure language mode\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/transformers/models/idefics/modeling_idefics.py:673\u001b[0m, in \u001b[0;36mIdeficsAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    670\u001b[0m     _, kv_len, _ \u001b[39m=\u001b[39m key_value_states\u001b[39m.\u001b[39msize()  \u001b[39m# Note that, in this case, `kv_len` == `kv_seq_len`\u001b[39;00m\n\u001b[1;32m    671\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(key_value_states)\u001b[39m.\u001b[39mview(bsz, kv_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    672\u001b[0m     value_states \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 673\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj(key_value_states)\u001b[39m.\u001b[39mview(bsz, kv_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m    674\u001b[0m     )\n\u001b[1;32m    676\u001b[0m kv_seq_len \u001b[39m=\u001b[39m key_states\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m    677\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:248\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    245\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[1;32m    247\u001b[0m bias \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 248\u001b[0m out \u001b[39m=\u001b[39m bnb\u001b[39m.\u001b[39;49mmatmul_4bit(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mt(), bias\u001b[39m=\u001b[39;49mbias, quant_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight\u001b[39m.\u001b[39;49mquant_state)\n\u001b[1;32m    250\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:579\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[39mreturn\u001b[39;00m out\n\u001b[1;32m    578\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 579\u001b[0m     \u001b[39mreturn\u001b[39;00m MatMul4Bit\u001b[39m.\u001b[39;49mapply(A, B, out, bias, quant_state)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/autograd/function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    537\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    538\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 539\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    543\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    544\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    545\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    546\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    547\u001b[0m     )\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mempty(A\u001b[39m.\u001b[39mshape[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m B_shape[:\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[39m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[39m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlinear(A, F\u001b[39m.\u001b[39;49mdequantize_4bit(B, state)\u001b[39m.\u001b[39mto(A\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[39m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[39m.\u001b[39mstate \u001b[39m=\u001b[39m state\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/bitsandbytes/functional.py:903\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m    901\u001b[0m \u001b[39mif\u001b[39;00m compressed_stats \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    902\u001b[0m     offset, state2 \u001b[39m=\u001b[39m compressed_stats\n\u001b[0;32m--> 903\u001b[0m     absmax \u001b[39m=\u001b[39m dequantize_blockwise(absmax, state2)\n\u001b[1;32m    904\u001b[0m     absmax \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m offset\n\u001b[1;32m    905\u001b[0m     \u001b[39mif\u001b[39;00m absmax\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mfloat32: absmax \u001b[39m=\u001b[39m absmax\u001b[39m.\u001b[39mfloat()\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/bitsandbytes/functional.py:710\u001b[0m, in \u001b[0;36mdequantize_blockwise\u001b[0;34m(A, quant_state, absmax, code, out, blocksize, nested)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBlockwise quantization only supports 16/32-bit floats, but got \u001b[39m\u001b[39m{\u001b[39;00mA\u001b[39m.\u001b[39mdtype\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 710\u001b[0m     post_call(A\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    711\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    712\u001b[0m     code \u001b[39m=\u001b[39m code\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/bitsandbytes/functional.py:421\u001b[0m, in \u001b[0;36mpost_call\u001b[0;34m(prev_device)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost_call\u001b[39m(prev_device):\n\u001b[0;32m--> 421\u001b[0m     torch\u001b[39m.\u001b[39;49mcuda\u001b[39m.\u001b[39;49mset_device(prev_device)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/cuda/__init__.py:402\u001b[0m, in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_device\u001b[39m(device: _device_t) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    393\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sets the current device.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m \n\u001b[1;32m    395\u001b[0m \u001b[39m    Usage of this function is discouraged in favor of :any:`device`. In most\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[39m            if this argument is negative.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     device \u001b[39m=\u001b[39m _get_device_index(device)\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    404\u001b[0m         torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_cuda_setDevice(device)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/cuda/_utils.py:39\u001b[0m, in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice):\n\u001b[1;32m     38\u001b[0m         \u001b[39mreturn\u001b[39;00m device\u001b[39m.\u001b[39midx\n\u001b[0;32m---> 39\u001b[0m \u001b[39mreturn\u001b[39;00m _torch_get_device_index(device, optional, allow_cpu)\n",
      "File \u001b[0;32m/mounts/work/pwicke/miniconda3/envs/idefics_colab/lib/python3.10/site-packages/torch/_utils.py:772\u001b[0m, in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_cpu \u001b[39mand\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    771\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a non cpu device, but got: \u001b[39m\u001b[39m{\u001b[39;00mdevice\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 772\u001b[0m     device_idx \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m device\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m device\u001b[39m.\u001b[39mindex\n\u001b[1;32m    773\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[1;32m    774\u001b[0m     device_idx \u001b[39m=\u001b[39m device\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_choices = dict()\n",
    "arrows = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "for action_word in action_words:\n",
    "    print(action_word)\n",
    "\n",
    "    # Creating list of images for processing\n",
    "    prompt = []\n",
    "    letters = [\"KYMD\", \"PWTX\", \"YZPR\", \"DHNV\"]\n",
    "    l = list(range(1,5))\n",
    "    random.shuffle(l)\n",
    "    for idx, selected in enumerate(l):\n",
    "        print(\"direction0\"+str(selected)+\" is \"+letters[idx])\n",
    "\n",
    "        prompt.append(Image.open(\"../../data/direction0\"+str(selected)+\".png\").convert(\"RGB\"))\n",
    "        prompt.append(\"Image \"+letters[idx]+\". \")\n",
    "\n",
    "    prompt.append(\" Question: Which of the images presents the event 'circle \"+action_word+\" square' best? Answer:\")\n",
    "    check_inference(model, processor, prompt, max_new_tokens=15)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fled\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction02 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'fled' is best represented by the image with code 'DHNV'. The image shows a bird flying away from a\n",
      "\n",
      "pointed at\n",
      "direction04 is KYMD\n",
      "direction02 is PWTX\n",
      "direction01 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'pointed at' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "pulled\n",
      "direction01 is KYMD\n",
      "direction04 is PWTX\n",
      "direction02 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'pulled' is best represented by the image with code 'DHNV'. The image shows a person pulling a rope\n",
      "\n",
      "pushed\n",
      "direction01 is KYMD\n",
      "direction02 is PWTX\n",
      "direction04 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'pushed' is best represented by the image with code DHNV. The event 'pushed' is best represented by\n",
      "\n",
      "walked\n",
      "direction02 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'walked' is best represented by the image with code 'DHNV'. The image is a stick figure walking. The\n",
      "\n",
      "hunted\n",
      "direction04 is KYMD\n",
      "direction02 is PWTX\n",
      "direction03 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'hunted' is best represented by the image with code 'DHNV'. The image is a square with a circle in\n",
      "\n",
      "impacted\n",
      "direction04 is KYMD\n",
      "direction02 is PWTX\n",
      "direction01 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'impacted' is best represented by the image with code 'DHNV'. The image shows a ball hitting a wall.\n",
      "\n",
      "perched\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction01 is YZPR\n",
      "direction02 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'perched' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "showed\n",
      "direction02 is KYMD\n",
      "direction04 is PWTX\n",
      "direction03 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'showed' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "smashed\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction02 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'smashed' is best represented by the image with code 'DHNV'. The image shows a ball hitting a wall and\n",
      "\n",
      "bombed\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction01 is YZPR\n",
      "direction02 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'bombed' is best represented by the image with code 'bombed'.\n",
      "\n",
      "flew\n",
      "direction02 is KYMD\n",
      "direction03 is PWTX\n",
      "direction04 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'flew' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "floated\n",
      "direction03 is KYMD\n",
      "direction02 is PWTX\n",
      "direction01 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'floated' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "lifted\n",
      "direction02 is KYMD\n",
      "direction01 is PWTX\n",
      "direction04 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'lifted' is best represented by the image with code 'DHNV'. The image shows a person lifting a box\n",
      "\n",
      "sank\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'sank' is best represented by the image with code:\n",
      "\n",
      "argued with\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction02 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'argued with' is best represented by the image with code 'DHNV'. The image with code 'KYMD'\n",
      "\n",
      "gave to\n",
      "direction03 is KYMD\n",
      "direction01 is PWTX\n",
      "direction02 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'gave to' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "offended\n",
      "direction04 is KYMD\n",
      "direction02 is PWTX\n",
      "direction01 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'offended' is best represented by the image with code 'DHNV'. The image is a square with a circle in\n",
      "\n",
      "rushed\n",
      "direction02 is KYMD\n",
      "direction04 is PWTX\n",
      "direction03 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'rushed' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "warned\n",
      "direction01 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'warned' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "owned\n",
      "direction04 is KYMD\n",
      "direction02 is PWTX\n",
      "direction03 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'owned' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "The event 'owned' is best\n",
      "\n",
      "regretted\n",
      "direction04 is KYMD\n",
      "direction02 is PWTX\n",
      "direction03 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'regretted' is best represented by the image with code 'DHNV'. The image is a square with a circle in\n",
      "\n",
      "rested\n",
      "direction04 is KYMD\n",
      "direction03 is PWTX\n",
      "direction02 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'rested' is best represented by the image with code 'DHNV'. The image is a square with a dot in\n",
      "\n",
      "tempted\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction02 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'tempted' is best represented by the image with code 'DHNV'. The image shows a person standing on a platform\n",
      "\n",
      "wanted\n",
      "direction02 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'wanted' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "hoped\n",
      "direction02 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'hoped' is best represented by the image with code 'DHNV'. The image shows a person standing on a platform\n",
      "\n",
      "increased\n",
      "direction03 is KYMD\n",
      "direction02 is PWTX\n",
      "direction04 is YZPR\n",
      "direction01 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'increased' is best represented by the image with code 'DHNV'.\n",
      "\n",
      "obeyed\n",
      "direction01 is KYMD\n",
      "direction02 is PWTX\n",
      "direction03 is YZPR\n",
      "direction04 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'obeyed' is best represented by the image with code 'DHNV'. The image shows a person standing on a platform\n",
      "\n",
      "respected\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction03 is YZPR\n",
      "direction02 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'respected' is best represented by the image with code 'DHNV'. The image is a square with a circle in\n",
      "\n",
      "succeeded\n",
      "direction04 is KYMD\n",
      "direction01 is PWTX\n",
      "direction02 is YZPR\n",
      "direction03 is DHNV\n",
      "Image KYMD. Image PWTX. Image YZPR. Image DHNV.The event 'succeeded' is best represented by the image with code 'DHNV'. The event 'failed' is best represented by\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_choices = dict()\n",
    "arrows = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "\n",
    "for action_word in action_words:\n",
    "    print(action_word)\n",
    "\n",
    "    # Creating list of images for processing\n",
    "    prompt = []\n",
    "    letters = [\"KYMD\", \"PWTX\", \"YZPR\", \"DHNV\"]\n",
    "    l = list(range(1,5))\n",
    "    random.shuffle(l)\n",
    "    for idx, selected in enumerate(l):\n",
    "        print(\"direction0\"+str(selected)+\" is \"+letters[idx])\n",
    "\n",
    "        prompt.append(Image.open(\"../../data/direction0\"+str(selected)+\".png\").convert(\"RGB\"))\n",
    "        prompt.append(\"Image \"+letters[idx]+\". \")\n",
    "\n",
    "    prompt.append(\" The event '\"+action_word+\"' is best represented by the image with code\")\n",
    "    check_inference(model, processor, prompt, max_new_tokens=15)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "schemas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
